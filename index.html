<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="chenxshuo">
<meta property="og:url" content="https://chenxshuo.cn/index.html">
<meta property="og:site_name" content="chenxshuo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="chenxshuo">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://chenxshuo.cn/">





  <title>chenxshuo</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">chenxshuo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://chenxshuo.cn/2019/07/05/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenshuo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenxshuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/05/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-05T15:07:47+08:00">
                2019-07-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://chenxshuo.cn/2018/11/02/YOLO-v1-Learning-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenshuo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenxshuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/02/YOLO-v1-Learning-Notes/" itemprop="url">YOLO  v1 Learning Notes</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-02T22:20:36+08:00">
                2018-11-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="YOLO-v1-Learning-Notes"><a href="#YOLO-v1-Learning-Notes" class="headerlink" title="YOLO  v1 Learning Notes"></a>YOLO  v1 Learning Notes</h1><p><em>there are some math formula having wrong formats, you can use the link : <a href="https://hackmd.io/s/r1-faEB37" target="_blank" rel="noopener">https://hackmd.io/s/r1-faEB37</a> to see the whole passage</em></p>
<h2 id="Unified-Detection"><a href="#Unified-Detection" class="headerlink" title="Unified Detection"></a>Unified Detection</h2><ul>
<li>Unify the separate components of object detection into one single neural network</li>
<li>enable end-to-end training and real-time speeds</li>
</ul>
<ol>
<li><p>Divide the image into $S \times S​$ grid</p>
</li>
<li><p>Each grid predicts $B$ bounding boxes and <strong>confidence scores</strong></p>
<p> Each grid also predicts $C$ conditional class probabilities  $Pr(Class_i | Object)$ </p>
<blockquote>
<p>confidence scores reflect how confident the model is that the box contains an object and how accurate it thinks the box is </p>
</blockquote>
<blockquote>
<p>$confidence\ scores = Pr(Object) * IOU^{truth}_{pred}$</p>
</blockquote>
<blockquote>
<p>$Pr(Object)=\left{\begin{aligned} 1\ no\ object \ 0 \ otherwise \ \end{aligned} \right.$</p>
</blockquote>
<blockquote>
<p>$IOU=intersection / union$ </p>
</blockquote>
</li>
<li><p>Each bounding box consists of 5 predictions : $x,y,w,h,confidence$ </p>
<p> <img src="https://cdn-images-1.medium.com/max/1600/1*oXSVP0HPVIaZqPpSinxsRQ.png" alt></p>
<p> <img src="https://cdn-images-1.medium.com/max/1600/1*_GR3J6_Zkq8c18ugBhQZew.png" alt></p>
<blockquote>
<p>$x,y$ : the center of the box relative to the bounds of the grid cell<br>$w,h$ : width and height of the bounding box relative to the whole image</p>
<p>$confidence: $ the IOU between the predicted box and any ground truth box</p>
</blockquote>
</li>
<li><p>At test time , $Pr(Class_i|Object) * Pr(Object) * IOU^{truth}<em>{ored} = Pr(Class_i) * IOU^{truth}</em>{pred}$</p>
<p> It shows both the probabilities of the class appearing in the box and how well the box fits</p>
</li>
<li><p>Finally , the shape of the prediction tensor  of a image is $S\times S \times (B \times 5 + C)$</p>
<p> Every grid has $B$ bounding boxes and every box has $5 $ targets which are $x,y,w,h,confidence$,and every box has $C$ conditional probabilities , and there are total $S \times S$ grid.</p>
</li>
<li><p>For example , the author set $S = 7, B = 2$ and the PASCAL VOC has $C = 20$, therefore the final prediction is a $7 * 7 * (2 * 5 + 20) = 7 * 7 * 30$ tensor.</p>
</li>
</ol>
<h2 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h2><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">┌────────────┬────────────────────────┬───────────────────┐</span><br><span class="line">│    Name    │        Filters         │ Output Dimension  │</span><br><span class="line">├────────────┼────────────────────────┼───────────────────┤</span><br><span class="line">| INPUT	     |<span class="number">448</span> x <span class="number">448</span> x <span class="number">3</span>           |-------------------|</span><br><span class="line">│ Conv <span class="number">1</span>     │ <span class="number">7</span> x <span class="number">7</span> x <span class="number">64</span>, stride=<span class="number">2</span>   │ <span class="number">224</span> x <span class="number">224</span> x <span class="number">64</span>    │</span><br><span class="line">│ <span class="keyword">Max</span> Pool <span class="number">1</span> │ <span class="number">2</span> x <span class="number">2</span>, stride=<span class="number">2</span>        │ <span class="number">112</span> x <span class="number">112</span> x <span class="number">64</span>    │</span><br><span class="line">│ Conv <span class="number">2</span>     │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">192</span>            │ <span class="number">112</span> x <span class="number">112</span> x <span class="number">192</span>   │</span><br><span class="line">│ <span class="keyword">Max</span> Pool <span class="number">2</span> │ <span class="number">2</span> x <span class="number">2</span>, stride=<span class="number">2</span>        │ <span class="number">56</span> x <span class="number">56</span> x <span class="number">192</span>     │</span><br><span class="line">│ Conv <span class="number">3</span>     │ <span class="number">1</span> x <span class="number">1</span> x <span class="number">128</span>            │ <span class="number">56</span> x <span class="number">56</span> x <span class="number">128</span>     │</span><br><span class="line">│ Conv <span class="number">4</span>     │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">256</span>            │ <span class="number">56</span> x <span class="number">56</span> x <span class="number">256</span>     │</span><br><span class="line">│ Conv <span class="number">5</span>     │ <span class="number">1</span> x <span class="number">1</span> x <span class="number">256</span>            │ <span class="number">56</span> x <span class="number">56</span> x <span class="number">256</span>     │</span><br><span class="line">│ Conv <span class="number">6</span>     │ <span class="number">1</span> x <span class="number">1</span> x <span class="number">512</span>            │ <span class="number">56</span> x <span class="number">56</span> x <span class="number">512</span>     │</span><br><span class="line">│ <span class="keyword">Max</span> Pool <span class="number">3</span> │ <span class="number">2</span> x <span class="number">2</span>, stride=<span class="number">2</span>        │ <span class="number">28</span> x <span class="number">28</span> x <span class="number">512</span>     │</span><br><span class="line">│ Conv <span class="number">7</span>     │ <span class="number">1</span> x <span class="number">1</span> x <span class="number">256</span>            │ <span class="number">28</span> x <span class="number">28</span> x <span class="number">256</span>     │</span><br><span class="line">│ Conv <span class="number">8</span>     │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">512</span>            │ <span class="number">28</span> x <span class="number">28</span> x <span class="number">512</span>     │</span><br><span class="line">│ Conv <span class="number">9</span>     │ <span class="number">1</span> x <span class="number">1</span> x <span class="number">256</span>            │ <span class="number">28</span> x <span class="number">28</span> x <span class="number">256</span>     │</span><br><span class="line">│ Conv <span class="number">10</span>    │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">512</span>            │ <span class="number">28</span> x <span class="number">28</span> x <span class="number">512</span>     │</span><br><span class="line">│ Conv <span class="number">11</span>    │ <span class="number">1</span> x <span class="number">1</span> x <span class="number">256</span>            │ <span class="number">28</span> x <span class="number">28</span> x <span class="number">256</span>     │</span><br><span class="line">│ Conv <span class="number">12</span>    │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">512</span>            │ <span class="number">28</span> x <span class="number">28</span> x <span class="number">512</span>     │</span><br><span class="line">│ Conv <span class="number">13</span>    │ <span class="number">1</span> x <span class="number">1</span> x <span class="number">256</span>            │ <span class="number">28</span> x <span class="number">28</span> x <span class="number">256</span>     │</span><br><span class="line">│ Conv <span class="number">14</span>    │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">512</span>            │ <span class="number">28</span> x <span class="number">28</span> x <span class="number">512</span>     │</span><br><span class="line">│ Conv <span class="number">15</span>    │ <span class="number">1</span> x <span class="number">1</span> x <span class="number">512</span>            │ <span class="number">28</span> x <span class="number">28</span> x <span class="number">512</span>     │</span><br><span class="line">│ Conv <span class="number">16</span>    │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">1024</span>           │ <span class="number">28</span> x <span class="number">28</span> x <span class="number">1024</span>    │</span><br><span class="line">│ <span class="keyword">Max</span> Pool <span class="number">4</span> │ <span class="number">2</span> x <span class="number">2</span>, stride=<span class="number">2</span>        │ <span class="number">14</span> x <span class="number">14</span> x <span class="number">1024</span>    │</span><br><span class="line">│ Conv <span class="number">17</span>    │ <span class="number">1</span> x <span class="number">1</span> x <span class="number">512</span>            │ <span class="number">14</span> x <span class="number">14</span> x <span class="number">512</span>     │</span><br><span class="line">│ Conv <span class="number">18</span>    │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">1024</span>           │ <span class="number">14</span> x <span class="number">14</span> x <span class="number">1024</span>    │</span><br><span class="line">│ Conv <span class="number">19</span>    │ <span class="number">1</span> x <span class="number">1</span> x <span class="number">512</span>            │ <span class="number">14</span> x <span class="number">14</span> x <span class="number">512</span>     │</span><br><span class="line">│ Conv <span class="number">20</span>    │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">1024</span>           │ <span class="number">14</span> x <span class="number">14</span> x <span class="number">1024</span>    │</span><br><span class="line">│ Conv <span class="number">21</span>    │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">1024</span>           │ <span class="number">14</span> x <span class="number">14</span> x <span class="number">1024</span>    │</span><br><span class="line">│ Conv <span class="number">22</span>    │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">1024</span>, stride=<span class="number">2</span> │ <span class="number">7</span> x <span class="number">7</span> x <span class="number">1024</span>      │</span><br><span class="line">│ Conv <span class="number">23</span>    │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">1024</span>           │ <span class="number">7</span> x <span class="number">7</span> x <span class="number">1024</span>      │</span><br><span class="line">│ Conv <span class="number">24</span>    │ <span class="number">3</span> x <span class="number">3</span> x <span class="number">1024</span>           │ <span class="number">7</span> x <span class="number">7</span> x <span class="number">1024</span>      │</span><br><span class="line">│ FC <span class="number">1</span>       │ -                      │ <span class="number">4096</span>              │</span><br><span class="line">│ FC <span class="number">2</span>       │ -                      │ <span class="number">7</span> x <span class="number">7</span> x <span class="number">30</span> (<span class="number">1470</span>) │</span><br><span class="line">└────────────┴────────────────────────┴───────────────────┘</span><br></pre></td></tr></table></figure>

<h2 id="The-Loss-Function"><a href="#The-Loss-Function" class="headerlink" title="The Loss Function"></a>The Loss Function</h2><p>There are three parts of the final Loss Function, in this article I use $L_i (i = 1,2,3)$ to denote each of them and the total Loss is $Loss = \sum_{i=1}^3 L_i$.</p>
<p><img src="https://pic4.zhimg.com/80/v2-f9af0b8094b35f7c2ab2179efb6f4c8c_hd.jpg" alt></p>
<h3 id="Previous"><a href="#Previous" class="headerlink" title="Previous"></a>Previous</h3><p>There are some special denotes in the formula.</p>
<p>$$\begin{equation}<br>\mathbb{1_{ij}^{obj}}=\left{<br>\begin{aligned}<br>1\  in\ situation A \<br>0 \ otherwise \<br>\end{aligned}<br>\right.<br>\end{equation}$$</p>
<p>A : If an object is present in grid cell $i$ and the $j\ th$ bounding box predictor is “responsible” for that prediction</p>
<p>responsible: <em>YOLO predicts multiple bounding boxes per grid cell. At training time we only want <strong>one bounding box</strong> predictor to be responsible for each object. We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth.</em></p>
<p>$\sum <em>{i=0}^{S^2}\sum</em>{j=0}^{B}$  : sum of the value for every bounding box in every gird.</p>
<h3 id="Part-1"><a href="#Part-1" class="headerlink" title="Part 1"></a>Part 1</h3><p>$L_1 = \lambda_{coord}* (\sum <em>{i=0}^{S^2}\sum</em>{j=0}^{B} \mathbb{1}<em>{ij}^{obj} [(x_i - \hat{x_i})^2 + (y_i - \hat{y_i})^2] +\\sum _{i=0}^{S^2}\sum</em>{j=0}^{B} \mathbb{1}_{ij}^{obj} [(\sqrt{w_i} - \sqrt{\hat{w_i}})^2 + (\sqrt{h_i} - \sqrt{\hat{h_i}})^2]   )$</p>
<p>$(x_i,y_i,w_i,h_i)$ : the prediction center ,height and width of the responsible bounding box in the $i\ th$ grid box </p>
<p>$(\hat{x_i},\hat{y_i},\hat{w_i},\hat{h_i})$: the ground truth </p>
<p>In this part, it calculates <strong>the total error of the bounding box</strong>.</p>
<p>As for the $\sqrt{} $ calculation, the author mentions that <em>sum-squared error also equally weights errors in large boxes and small boxes. Our error metric should reflect that small deviations in large boxes matter less than in small boxes. To partially address this we predict the square root of the bounding box width and height instead of the width and height directly.</em> In other words, suppose we have two ground truth with 100<em>100 and 10\</em>10 width and height, then the same 5 pixels deviation has different influence. Therefore authors use the square root to remedy this. </p>
<p><img src="https://img-blog.csdn.net/20171022153041262?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3pzMDkyNw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p>
<h3 id="Part-2"><a href="#Part-2" class="headerlink" title="Part 2"></a>Part 2</h3><p>$ L_2 = \sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}<em>{ij}^{obj}(C_i - \hat{C_i})^2 +  \lambda</em>{noobj} \sum_{i=0}^{S^2}\sum_{j=0}^B \mathbb{1}_{ij}^{noobj} (C_i - \hat{C_i})^2$</p>
<p>In this part, it calculates the <strong>total error of confidence</strong>. </p>
<p>$C_i$: the prediction confidence from network.</p>
<p>$\hat{C_i}$ : the IOU between the predicted box and ground truth box.</p>
<h3 id="Part-3"><a href="#Part-3" class="headerlink" title="Part 3"></a>Part 3</h3><p>$L_3 = \sum_{i=0}^{S^2}\mathbb{1}<em>i^{obj} \sum</em>{c \in classes}(p_i(c) - \hat{p_i}(c))^2$</p>
<p>$L_3$ calculates the overall loss of the conditional probability for all classes.</p>
<p>$p_i(c)$ : the prediction from network</p>
<p>$\hat{p_i}(c)$ : equals 1 when the grid cell contains the class c.</p>
<h2 id="lambda"><a href="#lambda" class="headerlink" title="$\lambda$"></a>$\lambda$</h2><p>The $\lambda_{coord} $ and $\lambda_{noobj}$ are just the weight of parts in loss function , which are to show the different importance of each part in the loss function. </p>
<p>For example, we want to put more weight on the localization error rather than the classification error, we can set the weight of the localization error to be $\lambda{coord} = 5$ and the weight of the classification error is just 1. It’s the same as the usage of $\lambda_{noobj}$—-to show the difference between the grid with objects and the others.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi. You Only Look Once: Unified, Real-Time Object Detection </p>
<p>[2] Understanding YOLO <a href="https://hackernoon.com/understanding-yolo-f5a74bbc7967" target="_blank" rel="noopener">https://hackernoon.com/understanding-yolo-f5a74bbc7967</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://chenxshuo.cn/2018/10/30/perceptron/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenshuo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenxshuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/30/perceptron/" itemprop="url">perceptron</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-30T22:03:27+08:00">
                2018-10-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="第一次作业-感知机"><a href="#第一次作业-感知机" class="headerlink" title="第一次作业 感知机"></a>第一次作业 感知机</h1><h2 id="实验内容简介"><a href="#实验内容简介" class="headerlink" title="实验内容简介"></a>实验内容简介</h2><p>本次实验通过python实现感知机模型，并使用sklearn中的鸢尾花数据进行数据分类。在理解原有代码的基础上，通过加入正则项，调整学习率等方法来尝试提高模型的泛化能力和准确度。</p>
<h2 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h2><p>代码分为库导入，Perceptron类和main函数三大部分。</p>
<h3 id="库导入部分"><a href="#库导入部分" class="headerlink" title="库导入部分"></a>库导入部分</h3><p>导入所需的numpy包，math包，sklearn中的数据，画图工具matplotlib</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mlp</span><br><span class="line">mlp.use(<span class="string">"Agg"</span>)</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br></pre></td></tr></table></figure>

<h3 id="Perceptron-Class"><a href="#Perceptron-Class" class="headerlink" title="Perceptron Class"></a>Perceptron Class</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment">#构造函数 设定学习率eta 和迭代次数iter</span></span><br><span class="line">    <span class="comment">#默认为0.01 和 20</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, eta=<span class="number">0.01</span>, n_iter=<span class="number">20</span>)</span>:</span></span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#拟合函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="comment">#初始化模型参数</span></span><br><span class="line">        <span class="comment">#wb为 权重和偏置项</span></span><br><span class="line">        <span class="comment">#theta1保存了w即权重的更新情况</span></span><br><span class="line">        <span class="comment">#theta2保存了b即偏置项的更新情况</span></span><br><span class="line">        <span class="comment">#errors_保存了每一次迭代的分错点的个数情况</span></span><br><span class="line">        self.wb = np.zeros(<span class="number">1</span> + X.shape[<span class="number">1</span>])</span><br><span class="line">        self.theta1 = []</span><br><span class="line">        self.theta2 = []</span><br><span class="line">        self.errors_ = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#开始迭代</span></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.n_iter):</span><br><span class="line">            i = i + <span class="number">1</span></span><br><span class="line">            errors = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> xi , yi <span class="keyword">in</span> zip(X, y):</span><br><span class="line">                <span class="comment"># core codes: update the weights and bias</span></span><br><span class="line">                <span class="comment"># 使用函数表示式 避免了判断的过程 函数式思想 避免判断 对学习率*2</span></span><br><span class="line">                update = self.eta * (yi - self.predict(xi))</span><br><span class="line">                self.wb[<span class="number">1</span>:] = self.wb[<span class="number">1</span>:] + update * xi</span><br><span class="line">                self.wb[<span class="number">0</span>] = self.wb[<span class="number">0</span>] + update</span><br><span class="line">                </span><br><span class="line">                <span class="comment">#errors 是分错的次数</span></span><br><span class="line">                errors += int(update != <span class="number">0.0</span>)</span><br><span class="line">                err_sum = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> xj, yj <span class="keyword">in</span> zip(X, y):</span><br><span class="line">                    <span class="keyword">if</span>(yj - self.predict(xj) != <span class="number">0</span>):</span><br><span class="line">                        err_sum += abs(yj - self.predict(xj))</span><br><span class="line">                        </span><br><span class="line">            <span class="comment"># 第一个权值更新的情况</span></span><br><span class="line">            self.theta1.append(self.wb[<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 第二个权值更新的情况</span></span><br><span class="line">            self.theta2.append(self.wb[<span class="number">2</span>])</span><br><span class="line">            <span class="comment"># 每一轮迭代错误点的个数的变化</span></span><br><span class="line">            self.errors_.append(errors)</span><br><span class="line">            </span><br><span class="line">        print(self.wb)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#计算空间中一点到平面中的距离</span></span><br><span class="line">    <span class="comment">#函数式编程思想 避免if判断</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">net_input</span><span class="params">(self, xi)</span>:</span></span><br><span class="line">        <span class="comment">#空间中一点到平面的距离</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(xi, self.wb[<span class="number">1</span>:]) + self.wb[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#根据距离的正负判断分类的结果</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, xi)</span>:</span></span><br><span class="line">        <span class="comment">#indicator 函数 sign函数</span></span><br><span class="line">        <span class="keyword">return</span> np.where(self.net_input(xi) &lt;= <span class="number">0.0</span> , <span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#画出样本点和模型的分类结果</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_decision_regions</span><span class="params">(self, X, y, resolution = <span class="number">0.02</span>)</span>:</span></span><br><span class="line">        colors = [<span class="string">"red"</span>,<span class="string">"blue"</span>]</span><br><span class="line">        markers = [<span class="string">"o"</span>,<span class="string">"x"</span>]</span><br><span class="line">        cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class="line">        x1_max, x1_min = max(X[:,<span class="number">0</span>])+ <span class="number">1</span>, min(X[:, <span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">        x2_max, x2_min = max(X[:,<span class="number">1</span>]) + <span class="number">1</span>, min(X[:,<span class="number">1</span>]) - <span class="number">1</span></span><br><span class="line">        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),</span><br><span class="line">                               np.arange(x2_min, x2_max, resolution))</span><br><span class="line">        Z = self.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class="line">        <span class="comment">#print("Z is :",Z)</span></span><br><span class="line">        </span><br><span class="line">        Z = Z.reshape(xx1.shape)</span><br><span class="line">        plt.contourf(xx1, xx2, Z, alpha=<span class="number">0.4</span>, cmap=cmap)</span><br><span class="line">        plt.xlim(xx1.min(), xx1.max())</span><br><span class="line">        plt.ylim(xx2.min(),xx2.max())</span><br><span class="line">        <span class="keyword">for</span> idx, cl <span class="keyword">in</span> enumerate(np.unique(y)):</span><br><span class="line">            plt.scatter(x=X[y==cl, <span class="number">0</span>], y=X[y==cl,<span class="number">1</span>],</span><br><span class="line">                        alpha=<span class="number">0.8</span>, c=cmap(idx),</span><br><span class="line">                        marker=markers[idx],label=cl)</span><br><span class="line">        <span class="comment">#plt.savefig("./perceptron.png",format="png")</span></span><br><span class="line">        plt.show()</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#画出errors随着迭代次数的上升的变化情况</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">painter3D</span><span class="params">(self, theta1, theta2, loss, XX, YY)</span>:</span></span><br><span class="line">        fig = plt.figure()</span><br><span class="line">        ax1 = Axes3D(fig)</span><br><span class="line">        x,y,z = theta1, theta2, loss</span><br><span class="line">        XXY = XX</span><br><span class="line">        lst = []</span><br><span class="line">        xa, ya = np.meshgrid(np.arange(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">0.5</span>), np.arange(<span class="number">-10</span>,<span class="number">10</span>,<span class="number">0.5</span>))</span><br><span class="line">        sums = xa - xa</span><br><span class="line">        print(sums.shape)</span><br><span class="line">        print(xa)</span><br><span class="line">        print(XXY[<span class="number">1</span>][<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, XXY.shape[<span class="number">0</span>], <span class="number">1</span>):</span><br><span class="line">            sums += np.abs((YY[k] - (xa * XXY[k][<span class="number">0</span>] + ya * XXY[k][<span class="number">1</span>])))</span><br><span class="line">        </span><br><span class="line">        ax1.plot_surface(xa, ya, sums, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>, cmap=plt.cm.jet, alpha=<span class="number">0.3</span>)</span><br><span class="line">        ax1.plot_wireframe(x,y,z, rstride=<span class="number">5</span>, cstride=<span class="number">5</span>)</span><br><span class="line">        ax1.set_xlabel(<span class="string">"theta1"</span>)</span><br><span class="line">        ax1.set_ylabel(<span class="string">"theta2"</span>)</span><br><span class="line">        ax1.set_zlabel(<span class="string">"loss"</span>)</span><br><span class="line">        plt.show()</span><br></pre></td></tr></table></figure>

<h3 id="main"><a href="#main" class="headerlink" title="main"></a>main</h3><p>首先加载数据，选取指定行数和列属性的特征数据和标签，之后进行训练集和测试集的划分。</p>
<p>然后实例化一个Perceptron对象，使用数据进行模型的拟合，然后输出错误点的个数变化，并可视化最终结果和损失函数的变化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#载入数据</span></span><br><span class="line">    iris = load_iris()</span><br><span class="line">    <span class="comment">#设定数据的数量和属性列</span></span><br><span class="line">    X = iris.data[:<span class="number">150</span>, [<span class="number">0</span>,<span class="number">1</span>]]</span><br><span class="line">    y = iris.target[:<span class="number">150</span>]</span><br><span class="line">    y = np.where( y == <span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#划分训练集和测试集划分比例为7:3</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#实例化Perceptron</span></span><br><span class="line">    ppn = Perceptron(<span class="number">0.1</span>,<span class="number">50</span>)</span><br><span class="line">    <span class="comment">#模型拟合</span></span><br><span class="line">    rst = ppn.fit(X_train, y_train)</span><br><span class="line">    <span class="comment">#print("theta1 : ",np.array(rst.theta1))</span></span><br><span class="line">    print(<span class="string">"Num of errors  is "</span>, rst.errors_)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#可视化结果</span></span><br><span class="line">    ppn.plot_decision_regions(X_test,y_test)</span><br><span class="line">    ppn.painter3D(np.array(rst.theta1).reshape(len(rst.theta1),<span class="number">1</span>), np.array(rst.theta2).reshape(len(rst.theta2),<span class="number">1</span>),np.array(rst.errors_).reshape(len(rst.errors_),<span class="number">1</span>), X, y)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment">#调用主函数</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>在使用前100个数据的前两列属性进行训练，设定学习率为0.1，迭代次数为20次，结果如下：</p>
<p><img src="/images/perceptron/1540273094630.png" alt="1540273094630"></p>
<p>可以看到得到最终的模型为$3.84x_1-8.06x_2-1.8$ ,最终20次迭代结束100个训练样本分错的个数是2个，模型的效果不错。</p>
<p>之后是对测试集和得到的结果进行可视化，可以看到模型把测试集较好区分成两部分，每一个点都正确分类。</p>
<p><img src="/images/perceptron/1540271351217.png" alt="1540271351217"></p>
<p>下图是对于loss的可视化可以看到loss在一步步向较低的位置移动，体现出梯度下降方法的优化能力 </p>
<p><img src="/images/perceptron/1540271378154.png" alt="1540271378154"></p>
<h2 id="模型探索，调整，优化"><a href="#模型探索，调整，优化" class="headerlink" title="模型探索，调整，优化"></a>模型探索，调整，优化</h2><h3 id="调整学习率"><a href="#调整学习率" class="headerlink" title="调整学习率"></a>调整学习率</h3><p>学习率是模型每一次更新参数的步长，学习率过大理论上会导致过拟合或者越过最小点，学习率过小会导致训练的时间过长，时间消耗更高。</p>
<h4 id="提高学习率"><a href="#提高学习率" class="headerlink" title="提高学习率"></a>提高学习率</h4><p>初始的学习率为0.1，设置学习率为1,迭代次数为20</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ppn = Perceptron(<span class="number">1</span>,<span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="/images/perceptron/1540275060722.png" alt="1540275060722"></p>
<p><img src="/images/perceptron/1540275077882.png" alt="1540275077882"></p>
<p>虽然最终的结果也比较不错，可是在每次迭代的错误数据的波动来看，可以发现波动比较大，在学习率是0.1时的迭代错误数量的变化如下：</p>
<p><img src="/images/perceptron/1540275269830.png" alt="1540275269830"></p>
<p>而当学习率为1时：</p>
<p><img src="/images/perceptron/1540275308649.png" alt="1540275308649"></p>
<p>可以看出波动十分的剧烈，这就是因为学习率太大，导致模型的每次更新太多，使得本来已经正确分类的点再次被错误分类。</p>
<h4 id="降低学习率"><a href="#降低学习率" class="headerlink" title="降低学习率"></a>降低学习率</h4><p>将学习率从0.1降低至0.01，结果如下：</p>
<p><img src="/images/perceptron/1540275423448.png" alt="1540275423448"></p>
<p>可以看到loss下降的比较缓慢，不过因为数据集比较简单，虽然在训练集上并未拟合成功，但是已经恰好可以正确分类测试集的数据。</p>
<p>学习率为0.01时的波动如下：</p>
<p><img src="/images/perceptron/1540275704173.png" alt="1540275704173"></p>
<p>可以看到下降的比较缓慢，没有学习率为0.1时下降的迅速。</p>
<h4 id="动态更新学习率"><a href="#动态更新学习率" class="headerlink" title="动态更新学习率"></a>动态更新学习率</h4><p>动态更新学习率也是一种选择，其基本思路是在训练的初始阶段使用比较大的学习率，随着训练的进行渐渐减小学习率。最一开始学习率比较大可以较快的接近最优点，随着训练的进行，降低学习率可以避免越过全局最优，更快拟合。</p>
<p>在实验中，初始学习率设置为1.0 每一次更新学习率下降0.05，更改Perceptron.fit()代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.eta = self.eta - <span class="number">0.05</span></span><br><span class="line"><span class="keyword">for</span> xi , yi <span class="keyword">in</span> zip(X, y):</span><br><span class="line">             <span class="comment"># core codes: update the weights and bias</span></span><br><span class="line">             <span class="comment"># 使用函数表示式 避免了判断的过程 函数式思想 避免判断 对学习率*2</span></span><br><span class="line">             update = self.eta * (yi - self.predict(xi))</span><br><span class="line">             self.wb[<span class="number">1</span>:] = self.wb[<span class="number">1</span>:] + update * xi</span><br><span class="line">             self.wb[<span class="number">0</span>] = self.wb[<span class="number">0</span>] + update</span><br></pre></td></tr></table></figure>

<p>得到结果如下：</p>
<p><img src="/images/perceptron/1540276193975.png" alt="1540276193975"></p>
<p>其loss的下降情况如图：</p>
<p><img src="/images/perceptron/1540276431899.png" alt="1540276431899"></p>
<p>可以看到属于一个比较理想的优化状态</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>将三种情况的波动情况统一展示：</p>
<p><img src="/images/perceptron/1540276494468.png" alt="1540276494468"></p>
<p>虽然仅仅20次迭代不过依然可以看到十分明显的差异，学习率比较小的情况下loss下降比较缓慢，学习率比较大的情况下loss的波动情况十分明显，设定合适的学习率和动态下降学习率两种方法得到的结果比较理想，一直以比较快速的速率下降。</p>
<h3 id="调整迭代次数"><a href="#调整迭代次数" class="headerlink" title="调整迭代次数"></a>调整迭代次数</h3><p>迭代次数也是训练模型中一个比较重要的参数，如果训练次数过大，会让训练的时间上升并且导致模型过拟合，如果训练次数过小导致模型未训练完成，两种情况都会降低模型的泛化性能，降低模型的准确度。</p>
<p>初始情况下，设定学习率为0.1，迭代次数为20次，此时统计训练时间和模型的泛化能力</p>
<p>更改main函数中的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start = time.time()</span><br><span class="line">rst = ppn.fit(X_train, y_train)</span><br><span class="line">end = time.time()</span><br><span class="line">print(<span class="string">"Cost: "</span>,end-start,<span class="string">" s"</span>)</span><br></pre></td></tr></table></figure>

<p>结果如图：</p>
<p><img src="/images/perceptron/1540277920629.png" alt="1540277920629"></p>
<p><img src="/images/perceptron/1540277927362.png" alt="1540277927362"></p>
<p>可以看到用时0.39s</p>
<h4 id="提高迭代次数"><a href="#提高迭代次数" class="headerlink" title="提高迭代次数"></a>提高迭代次数</h4><p>设定学习率为0.1，训练次数从20次调整到100次，比较时间的变化和模型的表现能力：</p>
<p><img src="/images/perceptron/1540278115813.png" alt="1540278115813"></p>
<p>可以看到用时1.7s 是刚刚的四倍左右，模型的效果与原模型相差不多。</p>
<p><img src="/images/perceptron/1540278278455.png" alt="1540278278455"></p>
<h4 id="降低迭代次数"><a href="#降低迭代次数" class="headerlink" title="降低迭代次数"></a>降低迭代次数</h4><p>降低迭代次数为5次，结果如下</p>
<p><img src="/images/perceptron/1540278379539.png" alt="1540278379539"></p>
<p>可以看到虽然时间变短了一半，可是模型的效果并不理想，也就是说模型还未训练成功。</p>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p>通过调整迭代次数，可以体会到迭代次数对于模型训练的意义，过多的迭代次数并不会提高模型的性能，反而会导致过拟合并浪费时间，反之太小的迭代次数会导致模型没有训练成功。</p>
<h3 id="增加数据量"><a href="#增加数据量" class="headerlink" title="增加数据量"></a>增加数据量</h3><p>在原实验中，仅仅使用了前100行的数据进行区分，训练的到的模型效果比较理想，现在尝试使用更多的数据进行模型的训练。</p>
<p>设置学习率为0.1，迭代次数为20，选择前150行的数据进行训练，结果如下：</p>
<p><img src="/images/perceptron/1540278740406.png" alt="1540278740406"></p>
<p><img src="/images/perceptron/1540279493292.png" alt="1540279493292"></p>
<p>可以看到分类的效果并不理想，在训练集中loss一直没有下降很多最后从可视化结果中看出陷入了局部最优点无法继续下降，对于测试集数据分类的效果也不理想，</p>
<p>考虑到20次的迭代次数可能未训练成功，提高迭代此时为100，结果如下：</p>
<p><img src="/images/perceptron/1540279047443.png" alt="1540279047443"></p>
<p><img src="/images/perceptron/1540279314013.png" alt="1540279314013"></p>
<p>可以看到模型训练的结果十分不理想，首先loss一直没有下降的趋势，其次在测试集上完全无法分类。</p>
<p>考虑到其原因，我认为可以从数据的分布和过拟合两个角度来探讨。</p>
<p>从数据的分布来看，可以从图中看出红色的点和蓝色的×号之间不是线性可分的关系，因为感知机模型能够成功的基础之一便是数据集线性可分，因此使用现在的数据从根本上无法得到准确率100%的感知机模型。其次从过拟合的角度来比较前后两次实验结果，反而在迭代次数为20次的时训练得到的模型在测试集上的表现较为良好，没有出现全部分为一类的情况，因此我认为随着训练次数的增加，模型陷入了过拟合，导致泛化能力大大下降。</p>
<h3 id="加入正则项"><a href="#加入正则项" class="headerlink" title="加入正则项"></a>加入正则项</h3><h4 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h4><p>使用L2范数作为正则化项则损失函数为：</p>
<p>$L(w,b) = -\sum y_i(w\cdot x_i + b) + \frac{\lambda }{2}||(w,b)||^2$</p>
<p>对$L(w,b)$求导：</p>
<p>$\nabla _wL(w,b) =-\sum y_ix_i + \lambda \sum w $</p>
<p>$\nabla _bL(w,b) =-\sum y_i+ \lambda b$</p>
<p>因此更新公式为：</p>
<p>$w \leftarrow w + \eta (y_ix_i - \lambda\sum w)$</p>
<p>$b \leftarrow b + \eta(y_i - \lambda b)$</p>
<h4 id="代码更改"><a href="#代码更改" class="headerlink" title="代码更改"></a>代码更改</h4><p>将Perceptron.fit()代码中的参数更新部分改为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">update = self.eta * (yi - self.predict(xi))</span><br><span class="line">self.wb[<span class="number">1</span>:] = self.wb[<span class="number">1</span>:] + update * xi - self.eta * self.lamb * np.sum(self.wb[<span class="number">1</span>:])</span><br><span class="line">self.wb[<span class="number">0</span>] = self.wb[<span class="number">0</span>] + update - self.eta * self.lamb * self.wb[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>其中self.lamb为超参数，默认设置为0.1</p>
<h4 id="实验比较"><a href="#实验比较" class="headerlink" title="实验比较"></a>实验比较</h4><p>迭代一百次添加正则化：</p>
<p><img src="/images/perceptron/1540280899310.png" alt="1540280899310"></p>
<p>迭代100次不添加正则化：</p>
<p><img src="/images/perceptron/1540283381393.png" alt="1540283381393"></p>
<p>可以体现出添加正则项之后，在测试集上模型表现的更好一些.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://chenxshuo.cn/2018/10/30/what-is-the-three-elements-of-statistical-machine-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenshuo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenxshuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/30/what-is-the-three-elements-of-statistical-machine-learning/" itemprop="url">what is the three elements of statistical machine learning?</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-30T21:35:29+08:00">
                2018-10-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="What-is-the-three-elements-of-Statistic-Machine-Learning"><a href="#What-is-the-three-elements-of-Statistic-Machine-Learning" class="headerlink" title="What is the three elements of Statistic Machine Learning?"></a>What is the three elements of Statistic Machine Learning?</h1><p>Basically ,there three elements of Statistic Machine Learning are <strong>the model</strong>, <strong>the learning strategy</strong> and <strong>the algorithm</strong>.</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>In supervised learning , <strong>model</strong> can be a conditional probability distribution $P(Y|X)$ which can predict the probability of each label $Y$ using the input $X$. Besides, model can also be a decision function  $y = f(x)$ which can directly predict the label $Y$ using the input information $X$.</p>
<p>All the potential models consist the <strong>hypothesis space</strong>, the models in hypothesis space are countless in general. We can use $\mathbb{F}$ to denote the hypothesis space, then $\mathbb{F} = {f | Y = f(X)}$ or $\mathbb{F} = {P | P(Y|X)}$.Usually $\mathbb{F}$ is a parameter vector $\theta ,\  \theta \in R^n$,the $R^n$ is the n-dimensional Euclid Space and can be called the <strong>parameter space</strong>.</p>
<p>In conclusion, the model can be  $P(Y|X)$ , $f(X)$  and can be described by a n-dimensional vector,all the potential models is called hypothesis space, all the potential parameter is called parameter space.</p>
<h2 id="Learning-Strategy"><a href="#Learning-Strategy" class="headerlink" title="Learning Strategy"></a>Learning Strategy</h2><p>The goal of statistic learning is to choose the best model from the hypothesis space, so the criterions of learning and choosing model is also an essential part. </p>
<p>In short, learning strategy tries to convert a learning problem to be a optimization problem. It tries to optimize the SRM function :</p>
<p>$min_{f\in F}\  \frac{1}{N} \sum_{i=1}^{N}L(y_i, f(x_i)  + \lambda J(f)$</p>
<h3 id="Loss-function-cost-function-VS-risk-function-expected-loss"><a href="#Loss-function-cost-function-VS-risk-function-expected-loss" class="headerlink" title="Loss function (cost function) VS risk function(expected loss)"></a>Loss function (cost function) VS risk function(expected loss)</h3><p>Frankly speaking , loss function $L(Y,f(X))$ describes the model’s performance in one single prediction but the risk function shows the performance in some general situation.</p>
<p>Some useful loss functions are as follows :</p>
<ul>
<li><p>0-1 loss function</p>
<p>$ L(Y,f(X)) = \left{ \begin{aligned} 1\ Y \neq f(X) \ 0\ Y = f(X) \ \end{aligned} \right. $</p>
</li>
<li><p>quadratic loss function</p>
<p>$L(Y,f(X)) = (Y - f(X))^2$</p>
</li>
<li><p>absolute loss function</p>
<p>$L(Y,f(X)) = |Y - f(X)|$</p>
</li>
<li><p>logarithmic loss function</p>
<p>$L(Y, P(Y|X)) = -log(P(Y|X))$</p>
</li>
</ul>
<p>For a specified training set, the <strong>empirical risk ** or **empirical loss</strong> is the average loss in the <em>training set</em> of a model $f(X)$ . Furthermore, the <strong>risk function</strong> or <strong>expected loss</strong> describes the loss about the <em>P(X,Y)</em>.</p>
<p>Theoretically, when there are infinite number of samples in the training set, the $R_{emp} = R_{exp}$. However there are not enough data in most cases, so we need to use some methods to modify the $R_{emp}$ .</p>
<h3 id="ERM-VS-SRM"><a href="#ERM-VS-SRM" class="headerlink" title="ERM VS SRM"></a>ERM VS SRM</h3><p><strong>ERM</strong> : Empirical risk minimization</p>
<p><strong>SRM</strong>: Structural risk minimization</p>
<p>After we get a training set , a bunch models and the loss function, then we can directly calculate the <strong>empirical risk</strong>：$min <em>{ f \in F}\  \frac{1}{N} \sum</em>{i=1}^{N}L(y_i, f(x_i))$ and find a best model. Maximum likelihood estimation is a kind of ERM methods.</p>
<p>When there are enough training data, the empirical risk minimization can perform well, in other words if we don’t have enough data the ERM will lead to <strong>over-fitting</strong>.</p>
<p>Here is the reason why we need <strong>SRM</strong> , SRM is just like <strong>regularzation</strong> . It adds a <strong>regularizer</strong> or <strong>penalty term</strong> after the ERM : $R_{srm}(f) =\frac{1}{N} \sum_{i=1}^{N}L(y_i, f(x_i)  + \lambda J(f)$. The value of $J(f)$ will increase if the model gets more complex.</p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Algorithm is the specific methods to choose a best model from the hypothesis space. In most cases, the learning algorithm is an optimization algorithm. </p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] 李航. 统计学习方法[M]. 北京:清华大学出版社, 2012. 6-10</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://chenxshuo.cn/2018/10/06/The-distance-between-a-point-and-a-hyperplane/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenshuo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenxshuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/06/The-distance-between-a-point-and-a-hyperplane/" itemprop="url">The distance between a point and a hyperplane</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-06T21:15:46+08:00">
                2018-10-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="The-distance-between-a-point-and-a-hyperplane"><a href="#The-distance-between-a-point-and-a-hyperplane" class="headerlink" title="The distance between a point and a hyperplane"></a>The distance between a point and a hyperplane</h1><p>Calculating the distance between a point and a hyperplane is a very fundamental need in many machine learning algorithm like perceptron ,SVM and so on.</p>
<p>Suppose we have an n-dimensional hyperplane $w\cdot x+b = 0​$ , where $x,w,b​$ are all $n​$ dimensional vectors and a point $x_0​$ . The distance between the hyperplane and the point is $\frac {|w \cdot x_0 + b|}{||w||} ​$, where $|w \cdot x_0 + b|​$ is the absolute value of $w \cdot x_0 +b​$and $||w||​$ is the $L_2 \ norm​$  of  $w​$ which is $\sqrt{\sum_{i=1}^n(w_i^2)}​$.</p>
<p>The derivation process is as follows. Suppose the projection of $x_0$ on the hyperplane is $x_1$ ,so $w \cdot x_1 + b = 0$,and $|w\cdot \overrightarrow  {x_0x_1} | = |w||\overrightarrow{x_0x_1}|cos\theta = |w||\overrightarrow{x_0x_1}| = ||w||d$ </p>
<p>Besides , $|w \cdot \overrightarrow{x_0x_1}| = |\sum_{i=1}^nw_i\cdot (x_0^i - x_1^i)| = |\sum_{i=1}^nw_ix_0^i - \sum_{i=1}^nw_iw_1^i| = |\sum_{i=1}^nw_ix_0^i + b| = |w\cdot x_0 + b|$ </p>
<p>Therefore, $||w||d = |w\cdot x_0 + b|$ and $d = \frac {|w \cdot x_0 + b|}{||w||}$</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://blog.csdn.net/Chaolei3/article/details/78853146" target="_blank" rel="noopener">https://blog.csdn.net/Chaolei3/article/details/78853146</a></p>
<p>[2] <a href="https://blog.csdn.net/fengshuiyue/article/details/43482533" target="_blank" rel="noopener">https://blog.csdn.net/fengshuiyue/article/details/43482533</a></p>
<p>[3] <a href="https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_plane" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_plane</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://chenxshuo.cn/2018/10/06/Using-frp-to-connect-the-Jupyter-Notebook-in-a-local-area-network-LAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenshuo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenxshuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/06/Using-frp-to-connect-the-Jupyter-Notebook-in-a-local-area-network-LAN/" itemprop="url">Using frp to connect the Jupyter Notebook in a local area network(LAN)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-06T13:34:03+08:00">
                2018-10-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Using-frp-to-connect-the-Jupyter-Notebook-in-a-local-area-network-LAN"><a href="#Using-frp-to-connect-the-Jupyter-Notebook-in-a-local-area-network-LAN" class="headerlink" title="Using frp to connect the Jupyter Notebook in a local area network(LAN)"></a>Using frp to connect the Jupyter Notebook in a local area network(LAN)</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Jupyter Notebook is a very popular web application that allows you to learn, take notes, run python codes and so on. It’s very convenient and powerful. In this blog, I’d like to introduce how to connect a Jupyter Notebook in a local area network.</p>
<p>In some cases, a powerful Linux Server doesn’t have a public IP address, it’s in a local area network and one of the computers in the LAN has the public IP but doesn’t have proper ability or environment to run codes. So we want to use the public IP to connect the powerful server in the LAN.</p>
<h2 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h2><p>I use a famous tools named frp. frp is a fast reverse proxy to help you expose a local server behind a NAT or firewall to the internet. It supports tcp, udp, http and https protocol when requests can be forwarded by domains to backward web services.</p>
<h3 id="Step-1-Download-frp"><a href="#Step-1-Download-frp" class="headerlink" title="Step 1 Download frp"></a>Step 1 Download frp</h3><p>You can download the release package from the <a href="https://github.com/fatedier/frp" target="_blank" rel="noopener">frp github homepage</a>. Then untar the package to your own folder on server(the computer that has the public IP) and client(the computer that doesn’t have the public IP).</p>
<h3 id="Step-2-modify-the-ini-file"><a href="#Step-2-modify-the-ini-file" class="headerlink" title="Step 2. modify the ini file"></a>Step 2. modify the ini file</h3><p>On server, you need to modify the <strong>frps.ini</strong> to indicate the port you need. </p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[common]</span><br><span class="line">bind_port = 7000</span><br><span class="line">vhost_http_port = 8000</span><br></pre></td></tr></table></figure>

<p>On client, you need to modify the <strong>frpc.ini</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[common]</span><br><span class="line">server_addr = the server ip</span><br><span class="line">server_port = the bind_port</span><br><span class="line"></span><br><span class="line">[web]</span><br><span class="line">type = http</span><br><span class="line">local_port = the_vhost_http_port</span><br><span class="line">custom_domains = the server ip</span><br></pre></td></tr></table></figure>

<h3 id="Step-3-run-frp"><a href="#Step-3-run-frp" class="headerlink" title="Step 3. run frp"></a>Step 3. run frp</h3><p>Run the frp on server</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./frps -c ./frps.ini</span><br></pre></td></tr></table></figure>

<p>Run the frp on client</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./frpc -c ./frpc.ini</span><br></pre></td></tr></table></figure>

<h3 id="Step-4-run-Jupyter-Notebook"><a href="#Step-4-run-Jupyter-Notebook" class="headerlink" title="Step 4. run Jupyter Notebook"></a>Step 4. run Jupyter Notebook</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter-notebook --port=the_vhost_http_port --ip=127.0.0.1 --no-browser</span><br></pre></td></tr></table></figure>

<p>Then paste the jupyter token to browser, press enter and Done!</p>
<h2 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h2><p>Sometimes due to the server and the client don’t have the same time, there will be a authorization timeout error. You can specify the authorization time in the frps.ini file.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] authorization timeout solved <a href="https://github.com/fatedier/frp/issues/510" target="_blank" rel="noopener">https://github.com/fatedier/frp/issues/510</a></p>
<p>[2] frp的内网穿透及外网访问内网jupyter-notebook的实现 <a href="http://frankchen.xyz/2017/11/12/ftp-using/" target="_blank" rel="noopener">http://frankchen.xyz/2017/11/12/ftp-using/</a></p>
<p>[3] frp github homepage <a href="https://github.com/fatedier/frp" target="_blank" rel="noopener">https://github.com/fatedier/frp</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://chenxshuo.cn/2018/10/02/Learning-PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenshuo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenxshuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/02/Learning-PyTorch/" itemprop="url">Learning PyTorch</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-02T00:07:40+08:00">
                2018-10-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Learning-PyTorch-WriteUp"><a href="#Learning-PyTorch-WriteUp" class="headerlink" title="Learning PyTorch WriteUp"></a>Learning PyTorch WriteUp</h1><ul>
<li>Some notes during learning PyTorch</li>
<li>from MILA PyTorch Tutorial</li>
</ul>
<h1 id="1-Introduction-to-the-torch-tensor-library"><a href="#1-Introduction-to-the-torch-tensor-library" class="headerlink" title="1. Introduction to the torch tensor library"></a>1. Introduction to the torch tensor library</h1><h2 id="Torch’s-numpy-equivalent-with-GPU"><a href="#Torch’s-numpy-equivalent-with-GPU" class="headerlink" title="Torch’s numpy equivalent with GPU"></a>Torch’s numpy equivalent with GPU</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<h2 id="Initialize-a-random-tensor"><a href="#Initialize-a-random-tensor" class="headerlink" title="Initialize a random tensor"></a>Initialize a random tensor</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor(<span class="number">5</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-8.9251e+19,  3.0698e-41,  5.7453e-44],
        [ 0.0000e+00,         nan,  0.0000e+00],
        [ 1.3733e-14,  6.4076e+07,  2.0706e-19],
        [ 7.3909e+22,  2.4176e-12,  1.1625e+33],
        [ 8.9605e-01,  1.1632e+33,  5.6003e-02]])</code></pre><h2 id="From-a-uniform-distribution"><a href="#From-a-uniform-distribution" class="headerlink" title="From a uniform distribution"></a>From a uniform distribution</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.9609, -0.4556,  0.1158],
        [-0.3132,  0.1930,  0.9076],
        [ 0.4740,  0.2442,  0.5060],
        [ 0.5245,  0.0922,  0.9319],
        [ 0.1282,  0.5840,  0.1264]])</code></pre><h2 id="Return-the-tensor’s-size-or-shape"><a href="#Return-the-tensor’s-size-or-shape" class="headerlink" title="Return the tensor’s size or shape"></a>Return the tensor’s size or shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.size()</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([3, 5])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.size()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<pre><code>3</code></pre><h2 id="Tensor-types"><a href="#Tensor-types" class="headerlink" title="Tensor types"></a>Tensor types</h2><p>you can see the <a href="https://pytorch.org/docs/master/tensors.html" target="_blank" rel="noopener">https://pytorch.org/docs/master/tensors.html</a></p>
<h2 id="Creation-from-lists-and-numpy"><a href="#Creation-from-lists-and-numpy" class="headerlink" title="Creation from lists and numpy"></a>Creation from lists and numpy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = torch.LongTensor([[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1, 3],
        [4, 5]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([2, 2])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.size()[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>2</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.type())</span><br></pre></td></tr></table></figure>

<pre><code>torch.LongTensor</code></pre><h3 id="cast-to-Numpy"><a href="#cast-to-Numpy" class="headerlink" title="cast to Numpy"></a>cast to Numpy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.numpy().dtype)</span><br></pre></td></tr></table></figure>

<pre><code>int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.numpy())</span><br></pre></td></tr></table></figure>

<pre><code>[[1 3]
 [4 5]]</code></pre><h3 id="Creation-from-Numpy"><a href="#Creation-from-Numpy" class="headerlink" title="Creation from Numpy"></a>Creation from Numpy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.from_numpy(np.random.randn(<span class="number">5</span>,<span class="number">3</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-1.2731, -0.7654, -1.2433],
        [ 1.1618,  1.5458,  1.0784],
        [-3.3339,  0.2076,  0.5256],
        [-1.0627, -1.3626, -0.5256],
        [-1.4767, -0.8483, -0.3982]], dtype=torch.float64)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.from_numpy(np.random.randn(<span class="number">5</span>,<span class="number">3</span>).astype(np.float32)))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.0658,  1.2705,  1.5947],
        [-0.6534,  1.6254, -1.6494],
        [ 0.8195,  0.5997, -0.7203],
        [-0.7378,  1.2314,  0.8628],
        [-1.1144,  1.2328, -0.7599]])</code></pre><h2 id="Simple-Mathematical-operations"><a href="#Simple-Mathematical-operations" class="headerlink" title="Simple Mathematical operations"></a>Simple Mathematical operations</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.8902, -0.9704,  0.9236,  0.3538,  0.9437],
        [-0.7820,  0.4077, -0.9641,  0.1301,  0.6817],
        [-0.4845,  0.3000, -0.1780, -0.0953, -0.6103]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = x*torch.randn(<span class="number">3</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.9836,  0.1232,  0.6336,  0.1021,  0.2546],
        [-0.3492,  0.0863, -0.6427, -0.0933, -0.5991],
        [ 0.9436, -0.3715, -0.0759, -0.0123, -0.4144]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.size()</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([3, 5])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = x * torch.from_numpy(np.arange(<span class="number">15</span>).reshape(<span class="number">3</span>,<span class="number">5</span>).astype(np.float32))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.0000, -0.9704,  1.8471,  1.0613,  3.7746],
        [-3.9099,  2.4462, -6.7487,  1.0407,  6.1355],
        [-4.8450,  3.3000, -2.1358, -1.2386, -8.5439]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = x / torch.from_numpy(np.arange(<span class="number">15</span>).reshape(<span class="number">3</span>,<span class="number">5</span>).astype(np.float32))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[   -inf, -0.9704,  0.4618,  0.1179,  0.2359],
        [-0.1564,  0.0680, -0.1377,  0.0163,  0.0757],
        [-0.0485,  0.0273, -0.0148, -0.0073, -0.0436]])</code></pre><h2 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([3, 5])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = x + torch.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-1.0342, -1.1144,  0.7796,  0.2098,  0.7997],
        [-2.5883, -1.3986, -2.7705, -1.6763, -1.1246],
        [-2.2020, -1.4175, -1.8955, -1.8128, -2.3278]])</code></pre><h2 id="Reshape"><a href="#Reshape" class="headerlink" title="Reshape"></a>Reshape</h2><ul>
<li>use <strong>view()</strong></li>
<li>use <strong>transpoese()</strong></li>
<li>use <strong>permute()</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = torch.randn(<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 10, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.size()[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<pre><code>15</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.view(<span class="number">50</span>,<span class="number">15</span>).size()</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).unsqueeze(<span class="number">-1</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 15, 1])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).unsqueeze(<span class="number">-1</span>).squeeze().size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.transpose(<span class="number">0</span>,<span class="number">1</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([10, 5, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.transpose(<span class="number">1</span>,<span class="number">2</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 15, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.transpose(<span class="number">0</span>,<span class="number">2</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([15, 10, 5])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 15, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([10, 15, 5])</code></pre><h2 id="Repeat"><a href="#Repeat" class="headerlink" title="Repeat"></a>Repeat</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 10, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).unsqueeze(<span class="number">1</span>).expand(<span class="number">50</span>,<span class="number">100</span>,<span class="number">15</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 100, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).unsqueeze(<span class="number">1</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 1, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).unsqueeze(<span class="number">1</span>).expand_as(torch.randn(<span class="number">50</span>,<span class="number">100</span>,<span class="number">15</span>)).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 100, 15])</code></pre><h2 id="Concatenate"><a href="#Concatenate" class="headerlink" title="Concatenate"></a>Concatenate</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 10, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.cat([y,y],<span class="number">2</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 10, 30])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.cat([y,y,y],<span class="number">0</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([15, 10, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test = torch.from_numpy(np.arange(<span class="number">8</span>).reshape(<span class="number">2</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(test)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 1, 2, 3],
        [4, 5, 6, 7]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.cat([test,test]))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 1, 2, 3],
        [4, 5, 6, 7],
        [0, 1, 2, 3],
        [4, 5, 6, 7]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.cat([test,test],<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 1, 2, 3, 0, 1, 2, 3],
        [4, 5, 6, 7, 4, 5, 6, 7]])</code></pre><h2 id="Advanced-Indexing"><a href="#Advanced-Indexing" class="headerlink" title="Advanced Indexing"></a>Advanced Indexing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.from_numpy(np.arange(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y[[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]],

        [[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y[[<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]]])</code></pre><h2 id="GPU-Support"><a href="#GPU-Support" class="headerlink" title="GPU Support"></a>GPU Support</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.cuda.HalfTensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = torch.cuda.HalfTensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.matmul(x,y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.1143,  0.3516,  0.2078,  0.3489, -0.1671],
        [ 0.7505,  0.0242, -0.7261,  0.3008, -0.8843],
        [-1.3154,  0.3438,  1.2920,  0.0421,  1.2314],
        [ 0.9307, -0.6084, -0.9990, -0.4595, -0.5630],
        [-0.0299, -0.4775, -0.3538, -0.1324,  0.4709]],
       device=&apos;cuda:0&apos;, dtype=torch.float16)</code></pre><h2 id="Switch-between-GPU-and-CPU"><a href="#Switch-between-GPU-and-CPU" class="headerlink" title="Switch between GPU and CPU"></a>Switch between GPU and CPU</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.FloatTensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.4481,  0.6421,  0.0732],
        [-0.4162, -0.7326, -0.1296],
        [-0.4186,  0.4492,  0.7672],
        [ 0.0939,  0.5855,  0.4610],
        [-0.0449, -0.3133,  0.7728]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = x.cuda(device=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.4481,  0.6421,  0.0732],
        [-0.4162, -0.7326, -0.1296],
        [-0.4186,  0.4492,  0.7672],
        [ 0.0939,  0.5855,  0.4610],
        [-0.0449, -0.3133,  0.7728]], device=&apos;cuda:0&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = x.cpu()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.4481,  0.6421,  0.0732],
        [-0.4162, -0.7326, -0.1296],
        [-0.4186,  0.4492,  0.7672],
        [ 0.0939,  0.5855,  0.4610],
        [-0.0449, -0.3133,  0.7728]])</code></pre><h1 id="Torch-Autograd-Variable-Define-by-run-amp-Execution-Paradigm"><a href="#Torch-Autograd-Variable-Define-by-run-amp-Execution-Paradigm" class="headerlink" title="Torch Autograd , Variable, Define-by-run &amp; Execution Paradigm"></a>Torch Autograd , Variable, Define-by-run &amp; Execution Paradigm</h1><h2 id="Variables-The-wrapper-around-tensors-to-facilitate-autograd"><a href="#Variables-The-wrapper-around-tensors-to-facilitate-autograd" class="headerlink" title="Variables: The wrapper around tensors to facilitate autograd"></a>Variables: The wrapper around tensors to facilitate autograd</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br></pre></td></tr></table></figure>

<h4 id="autograd-Variable-contains-data-grad-creator"><a href="#autograd-Variable-contains-data-grad-creator" class="headerlink" title="autograd.Variable contains data, grad, creator"></a>autograd.Variable contains data, grad, creator</h4><h3 id="Wrap-tensors-in-a-Variable"><a href="#Wrap-tensors-in-a-Variable" class="headerlink" title="Wrap tensors in a Variable"></a>Wrap tensors in a Variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.1203,  0.3902,  0.4211],
        [-0.7897, -0.4433,  0.8027],
        [ 0.1436,  0.4174, -0.9923],
        [-0.0598,  0.7010,  0.9009],
        [ 0.6464, -0.8007, -0.9541]])</code></pre><h3 id="Properties-of-Variables-Requiring-gradients-Volatility-Data-amp-Grad"><a href="#Properties-of-Variables-Requiring-gradients-Volatility-Data-amp-Grad" class="headerlink" title="Properties of Variables: Requiring gradients, Volatility, Data &amp; Grad"></a>Properties of Variables: Requiring gradients, Volatility, Data &amp; Grad</h3><ol>
<li>Access the raw tensor use .data attribute</li>
<li>Gradient of the loss w.r.t this variable is accumulated into .grad</li>
<li>Stay tuned for requires_grad and volatile</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Requires Gradient : %s"</span>%(z.requires_grad))</span><br></pre></td></tr></table></figure>

<pre><code>Requires Gradient : False</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z.data</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.1203,  0.3902,  0.4211],
        [-0.7897, -0.4433,  0.8027],
        [ 0.1436,  0.4174, -0.9923],
        [-0.0598,  0.7010,  0.9009],
        [ 0.6464, -0.8007, -0.9541]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Grad: %s"</span>%(z.grad))</span><br></pre></td></tr></table></figure>

<pre><code>Grad: None</code></pre><h3 id="Operations-on-variable"><a href="#Operations-on-variable" class="headerlink" title="Operations on variable"></a>Operations on variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">z = torch.mm(x,y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 5])</code></pre><h3 id="Mechanism-of-autograd"><a href="#Mechanism-of-autograd" class="headerlink" title="Mechanism of autograd"></a>Mechanism of autograd</h3><p>Autograd maintains a graph that records all of the operations performed on variables as you exeucte your operations.</p>
<p>A example:</p>
<p><img src="attachment:backprop.jpg" alt="backprop.jpg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.randn(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">x = x.cuda()</span><br><span class="line">y = Variable(torch.randn(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = y.cuda()</span><br><span class="line">z = torch.mm(x,y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.grad_fn)</span><br></pre></td></tr></table></figure>

<pre><code>&lt;MmBackward object at 0x7f774e959748&gt;</code></pre><h2 id="Getting-gradients-backward-amp-torch-autograd-grad"><a href="#Getting-gradients-backward-amp-torch-autograd-grad" class="headerlink" title="Getting gradients : backward() &amp; torch.autograd.grad"></a>Getting gradients : backward() &amp; torch.autograd.grad</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x ** <span class="number">2</span> + <span class="number">3</span> * y</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.3826, -2.2187, -1.2056],
        [-1.6237,  0.0499,  2.6776],
        [-0.6233,  1.2529, -2.0364],
        [ 0.4737,  1.0120, -0.0443],
        [-0.8420,  3.0115, -0.2205]], grad_fn=&lt;ThAddBackward&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z.backward(gradient=torch.ones(<span class="number">5</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<h3 id="Use-torch-eq-to-find-the-gradients-of-x"><a href="#Use-torch-eq-to-find-the-gradients-of-x" class="headerlink" title="Use torch.eq() to find the gradients of x"></a>Use torch.eq() to find the gradients of x</h3><p>torch.eq() computes element-wise equality</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(x.grad, <span class="number">2</span> * x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]], dtype=torch.uint8)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(y.grad,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]], dtype=torch.uint8)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_ = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y_ = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">z_ = x_ ** <span class="number">2</span> + y_ * <span class="number">3</span></span><br><span class="line">dz_dx = torch.autograd.grad(z_, x_, grad_outputs=torch.ones(<span class="number">5</span>,<span class="number">3</span>))</span><br><span class="line">dz_dy = torch.autograd.grad(z_, y_, grad_outputs=torch.ones(<span class="number">5</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dz_dx</span><br></pre></td></tr></table></figure>

<pre><code>(tensor([[-1.5474,  0.2862, -0.8905],
         [ 1.5274, -1.7896, -1.1907],
         [-1.2089,  0.7522, -0.4528],
         [ 0.6169, -1.9156, -0.5912],
         [ 1.6504, -0.5527, -1.1713]]),)</code></pre><h2 id="Define-by-run-example"><a href="#Define-by-run-example" class="headerlink" title="Define by run example"></a>Define by run example</h2><h3 id="Common-Variable-definition"><a href="#Common-Variable-definition" class="headerlink" title="Common Variable definition"></a>Common Variable definition</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">W = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">10</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = Variable(torch.Tensor(<span class="number">10</span>,).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Graph-1-xW-b"><a href="#Graph-1-xW-b" class="headerlink" title="Graph 1 : xW + b"></a>Graph 1 : xW + b</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = torch.matmul(x,W)+b</span><br><span class="line">do_dinputs_1 = torch.autograd.grad(o,[x,W,b],grad_outputs=torch.ones(<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Gradients of o w.r.t inputs in Graph 1'</span>)</span><br><span class="line">print(<span class="string">'do/dx : \n\n %s '</span> % (do_dinputs_1[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">'do/dw : \n\n %s '</span> % (do_dinputs_1[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">'do/db : \n\n %s '</span> % (do_dinputs_1[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>Gradients of o w.r.t inputs in Graph 1
do/dx : 

 tensor([[ 1.0566, -2.3974, -1.3943],
        [ 1.0566, -2.3974, -1.3943],
        [ 1.0566, -2.3974, -1.3943],
        [ 1.0566, -2.3974, -1.3943],
        [ 1.0566, -2.3974, -1.3943]]) 
do/dw : 

 tensor([[-1.4263, -1.4263, -1.4263, -1.4263, -1.4263, -1.4263, -1.4263, -1.4263,
         -1.4263, -1.4263],
        [-0.5715, -0.5715, -0.5715, -0.5715, -0.5715, -0.5715, -0.5715, -0.5715,
         -0.5715, -0.5715],
        [ 0.0822,  0.0822,  0.0822,  0.0822,  0.0822,  0.0822,  0.0822,  0.0822,
          0.0822,  0.0822]]) 
do/db : 

 tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]) </code></pre><h3 id="Graph-2-xW-b"><a href="#Graph-2-xW-b" class="headerlink" title="Graph 2: xW/b"></a>Graph 2: xW/b</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = torch.matmul(x,W)/b</span><br><span class="line">do_dinputs_2 = torch.autograd.grad(o,[x,W,b],grad_outputs=torch.ones(<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Gradients of o w.r.t inputs in Graph 2'</span>)</span><br><span class="line">print(<span class="string">'do/dx : \n\n %s '</span> % (do_dinputs_2[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">'do/dw : \n\n %s '</span> % (do_dinputs_2[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">'do/db : \n\n %s '</span> % (do_dinputs_2[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>Gradients of o w.r.t inputs in Graph 2
do/dx : 

 tensor([[-8.9442, -7.2449, -1.6292],
        [-8.9442, -7.2449, -1.6292],
        [-8.9442, -7.2449, -1.6292],
        [-8.9442, -7.2449, -1.6292],
        [-8.9442, -7.2449, -1.6292]]) 
do/dw : 

 tensor([[  7.5326,  -2.1635,   7.5158,  -5.0601, -33.7359,  -1.6989,   1.8152,
           2.2143,   3.0442,  -5.5103],
        [  3.0180,  -0.8668,   3.0113,  -2.0274, -13.5168,  -0.6807,   0.7273,
           0.8872,   1.2197,  -2.2078],
        [ -0.4340,   0.1246,  -0.4330,   0.2915,   1.9436,   0.0979,  -0.1046,
          -0.1276,  -0.1754,   0.3175]]) 
do/db : 

 tensor([  -5.5137,    3.3848,   40.4868,   -5.6851, -406.3335,   -0.0470,
           1.1332,   -3.3762,   -5.6009,    9.8781]) </code></pre><h2 id="Gradient-Buffers-backward-and-retain-graph-True"><a href="#Gradient-Buffers-backward-and-retain-graph-True" class="headerlink" title="Gradient Buffers : .backward() and retain_graph = True"></a>Gradient Buffers : .backward() and retain_graph = True</h2><ol>
<li>Calling .backward() clears the current computation graph</li>
<li>Once .backward() is called, intemediate variables used in the construction of the graph are removed</li>
<li>To retain a graph , use retain_graph = True. </li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = torch.mm(x,W)+b</span><br><span class="line">o.backward(torch.ones(<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = o ** <span class="number">3</span></span><br><span class="line">o.backward(torch.ones(<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<pre><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-40-fd18cc8cdc5e&gt; in &lt;module&gt;()
      1 o = o ** 3
----&gt; 2 o.backward(torch.ones(5,10))


~/anaconda3/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
     91                 products. Defaults to ``False``.
     92         &quot;&quot;&quot;
---&gt; 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)
     94 
     95     def register_hook(self, hook):


~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     88     Variable._execution_engine.run_backward(
     89         tensors, grad_tensors, retain_graph, create_graph,
---&gt; 90         allow_unreachable=True)  # allow_unreachable flag
     91 
     92 


RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = torch.mm(x, W) + b</span><br><span class="line">o.backward(torch.ones(<span class="number">5</span>,<span class="number">10</span>),retain_graph=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = o ** <span class="number">3</span></span><br><span class="line">o.backward(torch.ones(<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h4 id="Calling-backward-multiple-times-will-accumulate-gradients-into-grad-and-NOT-overwrite-them"><a href="#Calling-backward-multiple-times-will-accumulate-gradients-into-grad-and-NOT-overwrite-them" class="headerlink" title="Calling .backward() multiple times will accumulate gradients into .grad and NOT overwrite them"></a>Calling .backward() multiple times will accumulate gradients into .grad and NOT overwrite them</h4><h2 id="Exluding-subgraphs-from-backward-requires-grad-False-volatile-True-detach"><a href="#Exluding-subgraphs-from-backward-requires-grad-False-volatile-True-detach" class="headerlink" title="Exluding subgraphs from backward : requires_grad = False, volatile=True, detach"></a>Exluding subgraphs from backward : requires_grad = False, volatile=True, detach</h2><h3 id="requires-grad-False"><a href="#requires-grad-False" class="headerlink" title="requires_grad = False"></a>requires_grad = False</h3><ol>
<li><p>If there’s a single input to an operation that requires gradient, its output will also require gradient.</p>
</li>
<li><p>Conversely, if all inputs don’t require gradient, the output won’t require it.</p>
</li>
<li><p>Backward computation is never performed in the subgraphs, where all Variables didn’t require gradients.</p>
</li>
<li><p>This is potentially useful when you have part of a network that is pretrained and not fine-tuned, for example word embeddings or a pretrained imagenet model.</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">False</span>)</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">False</span>)</span><br><span class="line">z = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">o = x + y</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"o = x + y retuires grad ? : %s"</span>%(o.requires_grad))</span><br></pre></td></tr></table></figure>

<pre><code>o = x + y retuires grad ? : False</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">o = x + y + z</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"o = x + y retuires grad ? : %s"</span>%(o.requires_grad))</span><br></pre></td></tr></table></figure>

<pre><code>o = x + y retuires grad ? : True</code></pre><h3 id="volatile-True"><a href="#volatile-True" class="headerlink" title="volatile = True"></a>volatile = True</h3><ol>
<li>If a single input to an operation is volatile, the resulting variable will not have a grad_fn and so, the result will not be a node in the computation graph.</li>
<li>Conversely, only if all inputs are not volatile, the output will have a grad_fn and be included in the computation graph.</li>
<li>Volatile is useful when running Variables through your network during inference. Since it is fairly uncommon to go backwards through the network during inference, .backward() is rarely invoked. This means graphs are never cleared and hence it is common to run out of memory pretty quickly. Since operations on volatile variables are not recorded on the tape and therfore save memory</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), volatile = <span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), volatile = <span class="literal">True</span>)</span><br><span class="line">z = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<pre><code>/home/shixun7/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  &quot;&quot;&quot;Entry point for launching an IPython kernel.
/home/shixun7/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.</code></pre><h3 id="detach"><a href="#detach" class="headerlink" title=".detach()"></a>.detach()</h3><p>This could lead to disconnected graphs. In which case, PyTorch will only backpropagate gradients until the point of disconnections</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">z = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m1 = x + y</span><br><span class="line">m2 = z ** <span class="number">2</span></span><br><span class="line">m1 = m1.detach()</span><br><span class="line">m3 = m1 + m2</span><br><span class="line">m3.backward(torch.ones(<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"dm3/dx \n\n %s"</span>%(x.grad))</span><br><span class="line">print(<span class="string">"\ndm3/dy \n\n %s"</span>%(y.grad))</span><br><span class="line">print(<span class="string">"\ndm3/dz \n\n %s"</span>%(z.grad))</span><br></pre></td></tr></table></figure>

<pre><code>dm3/dx 

 None

dm3/dy 

 None

dm3/dz 

 tensor([[-0.8963,  1.6275,  0.1601, -1.1044,  0.6193],
        [ 0.5059, -0.4881, -0.1707, -1.6671,  0.8776],
        [-1.6945, -1.8586,  0.5326,  1.5711, -1.5853]])</code></pre><h3 id="Gradients-w-r-t-intermediate-variables"><a href="#Gradients-w-r-t-intermediate-variables" class="headerlink" title="Gradients w.r.t intermediate variables"></a>Gradients w.r.t intermediate variables</h3><ol>
<li>.retain_grad()</li>
<li>or explicitly compute gradients using torch.autograd.grad</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">z = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">m1 = x + y</span><br><span class="line">m2 = z ** <span class="number">2</span></span><br><span class="line">m1.retain_grad()</span><br><span class="line">m2.retain_grad()</span><br><span class="line">m3 = m1 + m2</span><br><span class="line">m3.backward(torch.ones(<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m1.grad</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m2.grad</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]])</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://chenxshuo.cn/2018/04/22/mind-maps-of-cnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenshuo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenxshuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/22/mind-maps-of-cnn/" itemprop="url">Mind-Map of some Classical CNN architecture and Algorithms</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-22T16:51:30+08:00">
                2018-04-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>Recently , I have been reading some classical CNN papers in the AI club.</p>
<p>This passage shows some mind-maps I made to introduce the main content of each paper.</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/04/22/mind-maps-of-cnn/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://chenxshuo.cn/2018/03/04/清醒思考的艺术/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenshuo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenxshuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/04/清醒思考的艺术/" itemprop="url">清醒思考的艺术</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-04T14:36:20+08:00">
                2018-03-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><img src="https://wallpapers.wallhaven.cc/wallpapers/full/wallhaven-438239.jpg" alt></p>
<p>清醒思考的艺术摘录</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/03/04/清醒思考的艺术/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://chenxshuo.cn/2018/03/04/小狗钱钱-摘录/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenshuo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenxshuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/04/小狗钱钱-摘录/" itemprop="url">小狗钱钱 摘录</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-04T14:28:15+08:00">
                2018-03-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><img src="https://wallpapers.wallhaven.cc/wallpapers/full/wallhaven-168443.jpg" alt></p>
<p>理财入门儿童读物《小狗钱钱》摘录</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/03/04/小狗钱钱-摘录/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">chenshuo</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chenshuo</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
