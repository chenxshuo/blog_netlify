<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="Learning PyTorch WriteUp Some notes during learning PyTorch from MILA PyTorch Tutorial  1. Introduction to the torch tensor libraryTorch’s numpy equivalent with GPU1import numpy as np  1import torch">
<meta property="og:type" content="article">
<meta property="og:title" content="Learning PyTorch">
<meta property="og:url" content="https://chenxshuo.cn/2018/10/02/Learning-PyTorch/index.html">
<meta property="og:site_name" content="chenxshuo">
<meta property="og:description" content="Learning PyTorch WriteUp Some notes during learning PyTorch from MILA PyTorch Tutorial  1. Introduction to the torch tensor libraryTorch’s numpy equivalent with GPU1import numpy as np  1import torch">
<meta property="og:locale" content="default">
<meta property="og:image" content="attachment:backprop.jpg">
<meta property="og:updated_time" content="2019-07-05T07:29:54.923Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning PyTorch">
<meta name="twitter:description" content="Learning PyTorch WriteUp Some notes during learning PyTorch from MILA PyTorch Tutorial  1. Introduction to the torch tensor libraryTorch’s numpy equivalent with GPU1import numpy as np  1import torch">
<meta name="twitter:image" content="attachment:backprop.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://chenxshuo.cn/2018/10/02/Learning-PyTorch/">





  <title>Learning PyTorch | chenxshuo</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">chenxshuo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://chenxshuo.cn/2018/10/02/Learning-PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chenshuo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chenxshuo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Learning PyTorch</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-02T00:07:40+08:00">
                2018-10-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Learning-PyTorch-WriteUp"><a href="#Learning-PyTorch-WriteUp" class="headerlink" title="Learning PyTorch WriteUp"></a>Learning PyTorch WriteUp</h1><ul>
<li>Some notes during learning PyTorch</li>
<li>from MILA PyTorch Tutorial</li>
</ul>
<h1 id="1-Introduction-to-the-torch-tensor-library"><a href="#1-Introduction-to-the-torch-tensor-library" class="headerlink" title="1. Introduction to the torch tensor library"></a>1. Introduction to the torch tensor library</h1><h2 id="Torch’s-numpy-equivalent-with-GPU"><a href="#Torch’s-numpy-equivalent-with-GPU" class="headerlink" title="Torch’s numpy equivalent with GPU"></a>Torch’s numpy equivalent with GPU</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<h2 id="Initialize-a-random-tensor"><a href="#Initialize-a-random-tensor" class="headerlink" title="Initialize a random tensor"></a>Initialize a random tensor</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor(<span class="number">5</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-8.9251e+19,  3.0698e-41,  5.7453e-44],
        [ 0.0000e+00,         nan,  0.0000e+00],
        [ 1.3733e-14,  6.4076e+07,  2.0706e-19],
        [ 7.3909e+22,  2.4176e-12,  1.1625e+33],
        [ 8.9605e-01,  1.1632e+33,  5.6003e-02]])</code></pre><h2 id="From-a-uniform-distribution"><a href="#From-a-uniform-distribution" class="headerlink" title="From a uniform distribution"></a>From a uniform distribution</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.9609, -0.4556,  0.1158],
        [-0.3132,  0.1930,  0.9076],
        [ 0.4740,  0.2442,  0.5060],
        [ 0.5245,  0.0922,  0.9319],
        [ 0.1282,  0.5840,  0.1264]])</code></pre><h2 id="Return-the-tensor’s-size-or-shape"><a href="#Return-the-tensor’s-size-or-shape" class="headerlink" title="Return the tensor’s size or shape"></a>Return the tensor’s size or shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.size()</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([3, 5])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.size()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<pre><code>3</code></pre><h2 id="Tensor-types"><a href="#Tensor-types" class="headerlink" title="Tensor types"></a>Tensor types</h2><p>you can see the <a href="https://pytorch.org/docs/master/tensors.html" target="_blank" rel="noopener">https://pytorch.org/docs/master/tensors.html</a></p>
<h2 id="Creation-from-lists-and-numpy"><a href="#Creation-from-lists-and-numpy" class="headerlink" title="Creation from lists and numpy"></a>Creation from lists and numpy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = torch.LongTensor([[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1, 3],
        [4, 5]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([2, 2])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.size()[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>2</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.type())</span><br></pre></td></tr></table></figure>

<pre><code>torch.LongTensor</code></pre><h3 id="cast-to-Numpy"><a href="#cast-to-Numpy" class="headerlink" title="cast to Numpy"></a>cast to Numpy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.numpy().dtype)</span><br></pre></td></tr></table></figure>

<pre><code>int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.numpy())</span><br></pre></td></tr></table></figure>

<pre><code>[[1 3]
 [4 5]]</code></pre><h3 id="Creation-from-Numpy"><a href="#Creation-from-Numpy" class="headerlink" title="Creation from Numpy"></a>Creation from Numpy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.from_numpy(np.random.randn(<span class="number">5</span>,<span class="number">3</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-1.2731, -0.7654, -1.2433],
        [ 1.1618,  1.5458,  1.0784],
        [-3.3339,  0.2076,  0.5256],
        [-1.0627, -1.3626, -0.5256],
        [-1.4767, -0.8483, -0.3982]], dtype=torch.float64)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.from_numpy(np.random.randn(<span class="number">5</span>,<span class="number">3</span>).astype(np.float32)))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.0658,  1.2705,  1.5947],
        [-0.6534,  1.6254, -1.6494],
        [ 0.8195,  0.5997, -0.7203],
        [-0.7378,  1.2314,  0.8628],
        [-1.1144,  1.2328, -0.7599]])</code></pre><h2 id="Simple-Mathematical-operations"><a href="#Simple-Mathematical-operations" class="headerlink" title="Simple Mathematical operations"></a>Simple Mathematical operations</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.8902, -0.9704,  0.9236,  0.3538,  0.9437],
        [-0.7820,  0.4077, -0.9641,  0.1301,  0.6817],
        [-0.4845,  0.3000, -0.1780, -0.0953, -0.6103]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = x*torch.randn(<span class="number">3</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.9836,  0.1232,  0.6336,  0.1021,  0.2546],
        [-0.3492,  0.0863, -0.6427, -0.0933, -0.5991],
        [ 0.9436, -0.3715, -0.0759, -0.0123, -0.4144]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.size()</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([3, 5])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = x * torch.from_numpy(np.arange(<span class="number">15</span>).reshape(<span class="number">3</span>,<span class="number">5</span>).astype(np.float32))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.0000, -0.9704,  1.8471,  1.0613,  3.7746],
        [-3.9099,  2.4462, -6.7487,  1.0407,  6.1355],
        [-4.8450,  3.3000, -2.1358, -1.2386, -8.5439]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = x / torch.from_numpy(np.arange(<span class="number">15</span>).reshape(<span class="number">3</span>,<span class="number">5</span>).astype(np.float32))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[   -inf, -0.9704,  0.4618,  0.1179,  0.2359],
        [-0.1564,  0.0680, -0.1377,  0.0163,  0.0757],
        [-0.0485,  0.0273, -0.0148, -0.0073, -0.0436]])</code></pre><h2 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([3, 5])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = x + torch.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-1.0342, -1.1144,  0.7796,  0.2098,  0.7997],
        [-2.5883, -1.3986, -2.7705, -1.6763, -1.1246],
        [-2.2020, -1.4175, -1.8955, -1.8128, -2.3278]])</code></pre><h2 id="Reshape"><a href="#Reshape" class="headerlink" title="Reshape"></a>Reshape</h2><ul>
<li>use <strong>view()</strong></li>
<li>use <strong>transpoese()</strong></li>
<li>use <strong>permute()</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = torch.randn(<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 10, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.size()[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>

<pre><code>15</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.view(<span class="number">50</span>,<span class="number">15</span>).size()</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).unsqueeze(<span class="number">-1</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 15, 1])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).unsqueeze(<span class="number">-1</span>).squeeze().size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.transpose(<span class="number">0</span>,<span class="number">1</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([10, 5, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.transpose(<span class="number">1</span>,<span class="number">2</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 15, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.transpose(<span class="number">0</span>,<span class="number">2</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([15, 10, 5])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 15, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([10, 15, 5])</code></pre><h2 id="Repeat"><a href="#Repeat" class="headerlink" title="Repeat"></a>Repeat</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 10, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).unsqueeze(<span class="number">1</span>).expand(<span class="number">50</span>,<span class="number">100</span>,<span class="number">15</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 100, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).unsqueeze(<span class="number">1</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 1, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.view(<span class="number">-1</span>,<span class="number">15</span>).unsqueeze(<span class="number">1</span>).expand_as(torch.randn(<span class="number">50</span>,<span class="number">100</span>,<span class="number">15</span>)).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([50, 100, 15])</code></pre><h2 id="Concatenate"><a href="#Concatenate" class="headerlink" title="Concatenate"></a>Concatenate</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 10, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.cat([y,y],<span class="number">2</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 10, 30])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.cat([y,y,y],<span class="number">0</span>).size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([15, 10, 15])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test = torch.from_numpy(np.arange(<span class="number">8</span>).reshape(<span class="number">2</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(test)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 1, 2, 3],
        [4, 5, 6, 7]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.cat([test,test]))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 1, 2, 3],
        [4, 5, 6, 7],
        [0, 1, 2, 3],
        [4, 5, 6, 7]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(torch.cat([test,test],<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 1, 2, 3, 0, 1, 2, 3],
        [4, 5, 6, 7, 4, 5, 6, 7]])</code></pre><h2 id="Advanced-Indexing"><a href="#Advanced-Indexing" class="headerlink" title="Advanced Indexing"></a>Advanced Indexing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.from_numpy(np.arange(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y[[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]],

        [[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y[[<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]]])</code></pre><h2 id="GPU-Support"><a href="#GPU-Support" class="headerlink" title="GPU Support"></a>GPU Support</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.cuda.HalfTensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = torch.cuda.HalfTensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.matmul(x,y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.1143,  0.3516,  0.2078,  0.3489, -0.1671],
        [ 0.7505,  0.0242, -0.7261,  0.3008, -0.8843],
        [-1.3154,  0.3438,  1.2920,  0.0421,  1.2314],
        [ 0.9307, -0.6084, -0.9990, -0.4595, -0.5630],
        [-0.0299, -0.4775, -0.3538, -0.1324,  0.4709]],
       device=&apos;cuda:0&apos;, dtype=torch.float16)</code></pre><h2 id="Switch-between-GPU-and-CPU"><a href="#Switch-between-GPU-and-CPU" class="headerlink" title="Switch between GPU and CPU"></a>Switch between GPU and CPU</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.FloatTensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.4481,  0.6421,  0.0732],
        [-0.4162, -0.7326, -0.1296],
        [-0.4186,  0.4492,  0.7672],
        [ 0.0939,  0.5855,  0.4610],
        [-0.0449, -0.3133,  0.7728]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = x.cuda(device=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.4481,  0.6421,  0.0732],
        [-0.4162, -0.7326, -0.1296],
        [-0.4186,  0.4492,  0.7672],
        [ 0.0939,  0.5855,  0.4610],
        [-0.0449, -0.3133,  0.7728]], device=&apos;cuda:0&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = x.cpu()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.4481,  0.6421,  0.0732],
        [-0.4162, -0.7326, -0.1296],
        [-0.4186,  0.4492,  0.7672],
        [ 0.0939,  0.5855,  0.4610],
        [-0.0449, -0.3133,  0.7728]])</code></pre><h1 id="Torch-Autograd-Variable-Define-by-run-amp-Execution-Paradigm"><a href="#Torch-Autograd-Variable-Define-by-run-amp-Execution-Paradigm" class="headerlink" title="Torch Autograd , Variable, Define-by-run &amp; Execution Paradigm"></a>Torch Autograd , Variable, Define-by-run &amp; Execution Paradigm</h1><h2 id="Variables-The-wrapper-around-tensors-to-facilitate-autograd"><a href="#Variables-The-wrapper-around-tensors-to-facilitate-autograd" class="headerlink" title="Variables: The wrapper around tensors to facilitate autograd"></a>Variables: The wrapper around tensors to facilitate autograd</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br></pre></td></tr></table></figure>

<h4 id="autograd-Variable-contains-data-grad-creator"><a href="#autograd-Variable-contains-data-grad-creator" class="headerlink" title="autograd.Variable contains data, grad, creator"></a>autograd.Variable contains data, grad, creator</h4><h3 id="Wrap-tensors-in-a-Variable"><a href="#Wrap-tensors-in-a-Variable" class="headerlink" title="Wrap tensors in a Variable"></a>Wrap tensors in a Variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.1203,  0.3902,  0.4211],
        [-0.7897, -0.4433,  0.8027],
        [ 0.1436,  0.4174, -0.9923],
        [-0.0598,  0.7010,  0.9009],
        [ 0.6464, -0.8007, -0.9541]])</code></pre><h3 id="Properties-of-Variables-Requiring-gradients-Volatility-Data-amp-Grad"><a href="#Properties-of-Variables-Requiring-gradients-Volatility-Data-amp-Grad" class="headerlink" title="Properties of Variables: Requiring gradients, Volatility, Data &amp; Grad"></a>Properties of Variables: Requiring gradients, Volatility, Data &amp; Grad</h3><ol>
<li>Access the raw tensor use .data attribute</li>
<li>Gradient of the loss w.r.t this variable is accumulated into .grad</li>
<li>Stay tuned for requires_grad and volatile</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Requires Gradient : %s"</span>%(z.requires_grad))</span><br></pre></td></tr></table></figure>

<pre><code>Requires Gradient : False</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z.data</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.1203,  0.3902,  0.4211],
        [-0.7897, -0.4433,  0.8027],
        [ 0.1436,  0.4174, -0.9923],
        [-0.0598,  0.7010,  0.9009],
        [ 0.6464, -0.8007, -0.9541]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Grad: %s"</span>%(z.grad))</span><br></pre></td></tr></table></figure>

<pre><code>Grad: None</code></pre><h3 id="Operations-on-variable"><a href="#Operations-on-variable" class="headerlink" title="Operations on variable"></a>Operations on variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">z = torch.mm(x,y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.size())</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 5])</code></pre><h3 id="Mechanism-of-autograd"><a href="#Mechanism-of-autograd" class="headerlink" title="Mechanism of autograd"></a>Mechanism of autograd</h3><p>Autograd maintains a graph that records all of the operations performed on variables as you exeucte your operations.</p>
<p>A example:</p>
<p><img src="attachment:backprop.jpg" alt="backprop.jpg"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.randn(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">x = x.cuda()</span><br><span class="line">y = Variable(torch.randn(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = y.cuda()</span><br><span class="line">z = torch.mm(x,y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(z.grad_fn)</span><br></pre></td></tr></table></figure>

<pre><code>&lt;MmBackward object at 0x7f774e959748&gt;</code></pre><h2 id="Getting-gradients-backward-amp-torch-autograd-grad"><a href="#Getting-gradients-backward-amp-torch-autograd-grad" class="headerlink" title="Getting gradients : backward() &amp; torch.autograd.grad"></a>Getting gradients : backward() &amp; torch.autograd.grad</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x ** <span class="number">2</span> + <span class="number">3</span> * y</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.3826, -2.2187, -1.2056],
        [-1.6237,  0.0499,  2.6776],
        [-0.6233,  1.2529, -2.0364],
        [ 0.4737,  1.0120, -0.0443],
        [-0.8420,  3.0115, -0.2205]], grad_fn=&lt;ThAddBackward&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z.backward(gradient=torch.ones(<span class="number">5</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<h3 id="Use-torch-eq-to-find-the-gradients-of-x"><a href="#Use-torch-eq-to-find-the-gradients-of-x" class="headerlink" title="Use torch.eq() to find the gradients of x"></a>Use torch.eq() to find the gradients of x</h3><p>torch.eq() computes element-wise equality</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(x.grad, <span class="number">2</span> * x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]], dtype=torch.uint8)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(y.grad,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]], dtype=torch.uint8)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_ = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y_ = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">z_ = x_ ** <span class="number">2</span> + y_ * <span class="number">3</span></span><br><span class="line">dz_dx = torch.autograd.grad(z_, x_, grad_outputs=torch.ones(<span class="number">5</span>,<span class="number">3</span>))</span><br><span class="line">dz_dy = torch.autograd.grad(z_, y_, grad_outputs=torch.ones(<span class="number">5</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dz_dx</span><br></pre></td></tr></table></figure>

<pre><code>(tensor([[-1.5474,  0.2862, -0.8905],
         [ 1.5274, -1.7896, -1.1907],
         [-1.2089,  0.7522, -0.4528],
         [ 0.6169, -1.9156, -0.5912],
         [ 1.6504, -0.5527, -1.1713]]),)</code></pre><h2 id="Define-by-run-example"><a href="#Define-by-run-example" class="headerlink" title="Define by run example"></a>Define by run example</h2><h3 id="Common-Variable-definition"><a href="#Common-Variable-definition" class="headerlink" title="Common Variable definition"></a>Common Variable definition</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">5</span>,<span class="number">3</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">W = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">10</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = Variable(torch.Tensor(<span class="number">10</span>,).uniform_(<span class="number">-1</span>,<span class="number">1</span>),requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Graph-1-xW-b"><a href="#Graph-1-xW-b" class="headerlink" title="Graph 1 : xW + b"></a>Graph 1 : xW + b</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = torch.matmul(x,W)+b</span><br><span class="line">do_dinputs_1 = torch.autograd.grad(o,[x,W,b],grad_outputs=torch.ones(<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Gradients of o w.r.t inputs in Graph 1'</span>)</span><br><span class="line">print(<span class="string">'do/dx : \n\n %s '</span> % (do_dinputs_1[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">'do/dw : \n\n %s '</span> % (do_dinputs_1[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">'do/db : \n\n %s '</span> % (do_dinputs_1[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>Gradients of o w.r.t inputs in Graph 1
do/dx : 

 tensor([[ 1.0566, -2.3974, -1.3943],
        [ 1.0566, -2.3974, -1.3943],
        [ 1.0566, -2.3974, -1.3943],
        [ 1.0566, -2.3974, -1.3943],
        [ 1.0566, -2.3974, -1.3943]]) 
do/dw : 

 tensor([[-1.4263, -1.4263, -1.4263, -1.4263, -1.4263, -1.4263, -1.4263, -1.4263,
         -1.4263, -1.4263],
        [-0.5715, -0.5715, -0.5715, -0.5715, -0.5715, -0.5715, -0.5715, -0.5715,
         -0.5715, -0.5715],
        [ 0.0822,  0.0822,  0.0822,  0.0822,  0.0822,  0.0822,  0.0822,  0.0822,
          0.0822,  0.0822]]) 
do/db : 

 tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.]) </code></pre><h3 id="Graph-2-xW-b"><a href="#Graph-2-xW-b" class="headerlink" title="Graph 2: xW/b"></a>Graph 2: xW/b</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = torch.matmul(x,W)/b</span><br><span class="line">do_dinputs_2 = torch.autograd.grad(o,[x,W,b],grad_outputs=torch.ones(<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Gradients of o w.r.t inputs in Graph 2'</span>)</span><br><span class="line">print(<span class="string">'do/dx : \n\n %s '</span> % (do_dinputs_2[<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">'do/dw : \n\n %s '</span> % (do_dinputs_2[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">'do/db : \n\n %s '</span> % (do_dinputs_2[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>Gradients of o w.r.t inputs in Graph 2
do/dx : 

 tensor([[-8.9442, -7.2449, -1.6292],
        [-8.9442, -7.2449, -1.6292],
        [-8.9442, -7.2449, -1.6292],
        [-8.9442, -7.2449, -1.6292],
        [-8.9442, -7.2449, -1.6292]]) 
do/dw : 

 tensor([[  7.5326,  -2.1635,   7.5158,  -5.0601, -33.7359,  -1.6989,   1.8152,
           2.2143,   3.0442,  -5.5103],
        [  3.0180,  -0.8668,   3.0113,  -2.0274, -13.5168,  -0.6807,   0.7273,
           0.8872,   1.2197,  -2.2078],
        [ -0.4340,   0.1246,  -0.4330,   0.2915,   1.9436,   0.0979,  -0.1046,
          -0.1276,  -0.1754,   0.3175]]) 
do/db : 

 tensor([  -5.5137,    3.3848,   40.4868,   -5.6851, -406.3335,   -0.0470,
           1.1332,   -3.3762,   -5.6009,    9.8781]) </code></pre><h2 id="Gradient-Buffers-backward-and-retain-graph-True"><a href="#Gradient-Buffers-backward-and-retain-graph-True" class="headerlink" title="Gradient Buffers : .backward() and retain_graph = True"></a>Gradient Buffers : .backward() and retain_graph = True</h2><ol>
<li>Calling .backward() clears the current computation graph</li>
<li>Once .backward() is called, intemediate variables used in the construction of the graph are removed</li>
<li>To retain a graph , use retain_graph = True. </li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = torch.mm(x,W)+b</span><br><span class="line">o.backward(torch.ones(<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = o ** <span class="number">3</span></span><br><span class="line">o.backward(torch.ones(<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<pre><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-40-fd18cc8cdc5e&gt; in &lt;module&gt;()
      1 o = o ** 3
----&gt; 2 o.backward(torch.ones(5,10))


~/anaconda3/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
     91                 products. Defaults to ``False``.
     92         &quot;&quot;&quot;
---&gt; 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)
     94 
     95     def register_hook(self, hook):


~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     88     Variable._execution_engine.run_backward(
     89         tensors, grad_tensors, retain_graph, create_graph,
---&gt; 90         allow_unreachable=True)  # allow_unreachable flag
     91 
     92 


RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = torch.mm(x, W) + b</span><br><span class="line">o.backward(torch.ones(<span class="number">5</span>,<span class="number">10</span>),retain_graph=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">o = o ** <span class="number">3</span></span><br><span class="line">o.backward(torch.ones(<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h4 id="Calling-backward-multiple-times-will-accumulate-gradients-into-grad-and-NOT-overwrite-them"><a href="#Calling-backward-multiple-times-will-accumulate-gradients-into-grad-and-NOT-overwrite-them" class="headerlink" title="Calling .backward() multiple times will accumulate gradients into .grad and NOT overwrite them"></a>Calling .backward() multiple times will accumulate gradients into .grad and NOT overwrite them</h4><h2 id="Exluding-subgraphs-from-backward-requires-grad-False-volatile-True-detach"><a href="#Exluding-subgraphs-from-backward-requires-grad-False-volatile-True-detach" class="headerlink" title="Exluding subgraphs from backward : requires_grad = False, volatile=True, detach"></a>Exluding subgraphs from backward : requires_grad = False, volatile=True, detach</h2><h3 id="requires-grad-False"><a href="#requires-grad-False" class="headerlink" title="requires_grad = False"></a>requires_grad = False</h3><ol>
<li><p>If there’s a single input to an operation that requires gradient, its output will also require gradient.</p>
</li>
<li><p>Conversely, if all inputs don’t require gradient, the output won’t require it.</p>
</li>
<li><p>Backward computation is never performed in the subgraphs, where all Variables didn’t require gradients.</p>
</li>
<li><p>This is potentially useful when you have part of a network that is pretrained and not fine-tuned, for example word embeddings or a pretrained imagenet model.</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">False</span>)</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">False</span>)</span><br><span class="line">z = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">o = x + y</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"o = x + y retuires grad ? : %s"</span>%(o.requires_grad))</span><br></pre></td></tr></table></figure>

<pre><code>o = x + y retuires grad ? : False</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">o = x + y + z</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"o = x + y retuires grad ? : %s"</span>%(o.requires_grad))</span><br></pre></td></tr></table></figure>

<pre><code>o = x + y retuires grad ? : True</code></pre><h3 id="volatile-True"><a href="#volatile-True" class="headerlink" title="volatile = True"></a>volatile = True</h3><ol>
<li>If a single input to an operation is volatile, the resulting variable will not have a grad_fn and so, the result will not be a node in the computation graph.</li>
<li>Conversely, only if all inputs are not volatile, the output will have a grad_fn and be included in the computation graph.</li>
<li>Volatile is useful when running Variables through your network during inference. Since it is fairly uncommon to go backwards through the network during inference, .backward() is rarely invoked. This means graphs are never cleared and hence it is common to run out of memory pretty quickly. Since operations on volatile variables are not recorded on the tape and therfore save memory</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), volatile = <span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), volatile = <span class="literal">True</span>)</span><br><span class="line">z = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<pre><code>/home/shixun7/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  &quot;&quot;&quot;Entry point for launching an IPython kernel.
/home/shixun7/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.</code></pre><h3 id="detach"><a href="#detach" class="headerlink" title=".detach()"></a>.detach()</h3><p>This could lead to disconnected graphs. In which case, PyTorch will only backpropagate gradients until the point of disconnections</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">z = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m1 = x + y</span><br><span class="line">m2 = z ** <span class="number">2</span></span><br><span class="line">m1 = m1.detach()</span><br><span class="line">m3 = m1 + m2</span><br><span class="line">m3.backward(torch.ones(<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"dm3/dx \n\n %s"</span>%(x.grad))</span><br><span class="line">print(<span class="string">"\ndm3/dy \n\n %s"</span>%(y.grad))</span><br><span class="line">print(<span class="string">"\ndm3/dz \n\n %s"</span>%(z.grad))</span><br></pre></td></tr></table></figure>

<pre><code>dm3/dx 

 None

dm3/dy 

 None

dm3/dz 

 tensor([[-0.8963,  1.6275,  0.1601, -1.1044,  0.6193],
        [ 0.5059, -0.4881, -0.1707, -1.6671,  0.8776],
        [-1.6945, -1.8586,  0.5326,  1.5711, -1.5853]])</code></pre><h3 id="Gradients-w-r-t-intermediate-variables"><a href="#Gradients-w-r-t-intermediate-variables" class="headerlink" title="Gradients w.r.t intermediate variables"></a>Gradients w.r.t intermediate variables</h3><ol>
<li>.retain_grad()</li>
<li>or explicitly compute gradients using torch.autograd.grad</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">z = Variable(torch.Tensor(<span class="number">3</span>,<span class="number">5</span>).uniform_(<span class="number">-1</span>,<span class="number">1</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">m1 = x + y</span><br><span class="line">m2 = z ** <span class="number">2</span></span><br><span class="line">m1.retain_grad()</span><br><span class="line">m2.retain_grad()</span><br><span class="line">m3 = m1 + m2</span><br><span class="line">m3.backward(torch.ones(<span class="number">3</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m1.grad</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m2.grad</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1.]])</code></pre>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/22/mind-maps-of-cnn/" rel="next" title="Mind-Map of some Classical CNN architecture and Algorithms">
                <i class="fa fa-chevron-left"></i> Mind-Map of some Classical CNN architecture and Algorithms
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/10/06/Using-frp-to-connect-the-Jupyter-Notebook-in-a-local-area-network-LAN/" rel="prev" title="Using frp to connect the Jupyter Notebook in a local area network(LAN)">
                Using frp to connect the Jupyter Notebook in a local area network(LAN) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">chenshuo</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Learning-PyTorch-WriteUp"><span class="nav-number">1.</span> <span class="nav-text">Learning PyTorch WriteUp</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Introduction-to-the-torch-tensor-library"><span class="nav-number">2.</span> <span class="nav-text">1. Introduction to the torch tensor library</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Torch’s-numpy-equivalent-with-GPU"><span class="nav-number">2.1.</span> <span class="nav-text">Torch’s numpy equivalent with GPU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Initialize-a-random-tensor"><span class="nav-number">2.2.</span> <span class="nav-text">Initialize a random tensor</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#From-a-uniform-distribution"><span class="nav-number">2.3.</span> <span class="nav-text">From a uniform distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Return-the-tensor’s-size-or-shape"><span class="nav-number">2.4.</span> <span class="nav-text">Return the tensor’s size or shape</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor-types"><span class="nav-number">2.5.</span> <span class="nav-text">Tensor types</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Creation-from-lists-and-numpy"><span class="nav-number">2.6.</span> <span class="nav-text">Creation from lists and numpy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cast-to-Numpy"><span class="nav-number">2.6.1.</span> <span class="nav-text">cast to Numpy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Creation-from-Numpy"><span class="nav-number">2.6.2.</span> <span class="nav-text">Creation from Numpy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Simple-Mathematical-operations"><span class="nav-number">2.7.</span> <span class="nav-text">Simple Mathematical operations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Broadcasting"><span class="nav-number">2.8.</span> <span class="nav-text">Broadcasting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reshape"><span class="nav-number">2.9.</span> <span class="nav-text">Reshape</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Repeat"><span class="nav-number">2.10.</span> <span class="nav-text">Repeat</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Concatenate"><span class="nav-number">2.11.</span> <span class="nav-text">Concatenate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Advanced-Indexing"><span class="nav-number">2.12.</span> <span class="nav-text">Advanced Indexing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU-Support"><span class="nav-number">2.13.</span> <span class="nav-text">GPU Support</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Switch-between-GPU-and-CPU"><span class="nav-number">2.14.</span> <span class="nav-text">Switch between GPU and CPU</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Torch-Autograd-Variable-Define-by-run-amp-Execution-Paradigm"><span class="nav-number">3.</span> <span class="nav-text">Torch Autograd , Variable, Define-by-run &amp; Execution Paradigm</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Variables-The-wrapper-around-tensors-to-facilitate-autograd"><span class="nav-number">3.1.</span> <span class="nav-text">Variables: The wrapper around tensors to facilitate autograd</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#autograd-Variable-contains-data-grad-creator"><span class="nav-number">3.1.0.1.</span> <span class="nav-text">autograd.Variable contains data, grad, creator</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Wrap-tensors-in-a-Variable"><span class="nav-number">3.1.1.</span> <span class="nav-text">Wrap tensors in a Variable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Properties-of-Variables-Requiring-gradients-Volatility-Data-amp-Grad"><span class="nav-number">3.1.2.</span> <span class="nav-text">Properties of Variables: Requiring gradients, Volatility, Data &amp; Grad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Operations-on-variable"><span class="nav-number">3.1.3.</span> <span class="nav-text">Operations on variable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mechanism-of-autograd"><span class="nav-number">3.1.4.</span> <span class="nav-text">Mechanism of autograd</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Getting-gradients-backward-amp-torch-autograd-grad"><span class="nav-number">3.2.</span> <span class="nav-text">Getting gradients : backward() &amp; torch.autograd.grad</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Use-torch-eq-to-find-the-gradients-of-x"><span class="nav-number">3.2.1.</span> <span class="nav-text">Use torch.eq() to find the gradients of x</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Define-by-run-example"><span class="nav-number">3.3.</span> <span class="nav-text">Define by run example</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Common-Variable-definition"><span class="nav-number">3.3.1.</span> <span class="nav-text">Common Variable definition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Graph-1-xW-b"><span class="nav-number">3.3.2.</span> <span class="nav-text">Graph 1 : xW + b</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Graph-2-xW-b"><span class="nav-number">3.3.3.</span> <span class="nav-text">Graph 2: xW/b</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Buffers-backward-and-retain-graph-True"><span class="nav-number">3.4.</span> <span class="nav-text">Gradient Buffers : .backward() and retain_graph = True</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Calling-backward-multiple-times-will-accumulate-gradients-into-grad-and-NOT-overwrite-them"><span class="nav-number">3.4.0.1.</span> <span class="nav-text">Calling .backward() multiple times will accumulate gradients into .grad and NOT overwrite them</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#Exluding-subgraphs-from-backward-requires-grad-False-volatile-True-detach"><span class="nav-number">3.5.</span> <span class="nav-text">Exluding subgraphs from backward : requires_grad = False, volatile=True, detach</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#requires-grad-False"><span class="nav-number">3.5.1.</span> <span class="nav-text">requires_grad = False</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#volatile-True"><span class="nav-number">3.5.2.</span> <span class="nav-text">volatile = True</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#detach"><span class="nav-number">3.5.3.</span> <span class="nav-text">.detach()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradients-w-r-t-intermediate-variables"><span class="nav-number">3.5.4.</span> <span class="nav-text">Gradients w.r.t intermediate variables</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chenshuo</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  

</body>
</html>
